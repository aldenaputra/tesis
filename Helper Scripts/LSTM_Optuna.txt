Notebook: LSTM_Optuna.ipynb
Kernelspec: {'display_name': 'tesis_env', 'language': 'python', 'name': 'python3'}

--------------------------------------------------------------------------------
Cell 1
Cell type: markdown
-- Markdown --
# LSTM Hyperparameter Optimized
--------------------------------------------------------------------------------
Cell 2
Cell type: code
-- Code --
!pip install -q optuna optuna-integration[tfkeras]
-- Outputs --
[1] output_type: stream

[notice] A new release of pip is available: 23.1.2 -> 25.3
[notice] To update, run: python.exe -m pip install --upgrade pip
--------------------------------------------------------------------------------
Cell 3
Cell type: code
-- Code --
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import optuna
import random
import os

from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

import tensorflow as tf
from tensorflow.keras import Sequential, Input, optimizers
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from optuna.integration import TFKerasPruningCallback
--------------------------------------------------------------------------------
Cell 4
Cell type: code
-- Code --
file_name = ".SEED.txt"
with open(file_name, "r") as file:
    content = file.read().strip()  # Read and remove any extra whitespace/newlines
    number = int(content)  # Use float() to support decimal; use int() if it's always an integer

print("Seed:", number)
print("Type:", type(number))
-- Outputs --
[1] output_type: stream
Seed: 271828183
Type: <class 'int'>
--------------------------------------------------------------------------------
Cell 5
Cell type: code
-- Code --
CSV_PATH   = "ALL_MERGED.csv"     # <--- set your CSV path
DATE_COL   = "Date"
TARGET_COL = "JKSE"
INCLUDE_TARGET_AS_FEATURE = True  # set False to exclude JKSE from X

# fixed split ratios (you can change)
TEST_SIZE  = 0.20
VAL_SIZE   = 0.10

# search budget
N_TRIALS   = 50                  # increase for better search
RANDOM_SEED = number

print(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)
-- Outputs --
[1] output_type: stream
271828183
--------------------------------------------------------------------------------
Cell 6
Cell type: code
-- Code --
df = pd.read_csv(CSV_PATH)

required = [
    "Date","Nickel_Fut","Coal_Fut_Newcastle","Palm_Oil_Fut",
    "USD_IDR","CNY_IDR","EUR_IDR","BTC_USD",
    "FTSE100","HANGSENG","NIKKEI225","SNP500","DOW30","SSE_Composite","JKSE"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns: {missing}")

df[DATE_COL] = pd.to_datetime(df[DATE_COL])
df = df.sort_values(DATE_COL).set_index(DATE_COL)
df = df.ffill().bfill()
df
-- Outputs --
[1] output_type: execute_result
            Nickel_Fut  Coal_Fut_Newcastle  Palm_Oil_Fut  USD_IDR  CNY_IDR  \
Date                                                                         
2015-05-01    13750.00               62.20          2136  12962.5  2089.78   
2015-05-02    13750.00               62.20          2136  12962.5  2089.78   
2015-05-03    13750.00               62.20          2136  12962.5  2089.78   
2015-05-04    13750.00               62.20          2136  12980.0  2090.51   
2015-05-05    14245.00               62.50          2136  13047.5  2102.39   
...                ...                 ...           ...      ...      ...   
2025-04-27    15482.88               93.75          4057  16830.0  2309.37   
2025-04-28    15637.63               95.60          3943  16855.0  2310.59   
2025-04-29    15564.00               97.25          3911  16760.0  2305.52   
2025-04-30    15342.50               97.50          3911  16600.0  2282.94   
2025-05-01    15218.63               97.50          3881  16600.0  2282.94   

            EUR_IDR       BTC_USD      FTSE100     HANGSENG    NIKKEI225  \
Date                                                                       
2015-05-01  14519.9    232.078995  6986.000000  28123.82031  19531.63086   
2015-05-02  14519.9    234.929993  6986.000000  28123.82031  19531.63086   
2015-05-03  14519.9    240.358002  6986.000000  28123.82031  19531.63086   
2015-05-04  14467.5    239.018005  6986.000000  28123.82031  19531.63086   
2015-05-05  14594.9    236.121002  6927.600098  27755.53906  19531.63086   
...             ...           ...          ...          ...          ...   
2025-04-27  19127.3  93754.843750  8415.299805  21980.74023  35705.73828   
2025-04-28  19251.8  94978.750000  8417.299805  21971.96094  35839.98828   
2025-04-29  19082.1  94284.789060  8463.500000  22008.10938  35839.98828   
2025-04-30  18805.3  94207.312500  8494.900391  22119.41016  36045.37891   
2025-05-01  18745.6  96492.335940  8496.799805  22119.41016  36452.30078   

                 SNP500        DOW30  SSE_Composite         JKSE  
Date                                                              
2015-05-01  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-02  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-03  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-04  2114.489990  18070.40039    4480.463867  5140.937012  
2015-05-05  2089.459961  17928.19922    4298.706055  5160.107422  
...                 ...          ...            ...          ...  
2025-04-27  5525.209961  40113.50000    3295.060059  6678.915039  
2025-04-28  5528.750000  40227.58984    3288.415039  6722.965820  
2025-04-29  5560.830078  40527.62109    3286.655029  6749.075195  
2025-04-30  5569.060059  40669.35938    3279.031006  6766.794922  
2025-05-01  5604.140137  40752.96094    3279.031006  6766.794922  

[3654 rows x 14 columns]
--------------------------------------------------------------------------------
Cell 7
Cell type: code
-- Code --
plt.figure(figsize=(12, 7))
plt.plot(df.index, df)
plt.title(f"Dataset Plot")
plt.xlabel("Date"); plt.ylabel("Price")
plt.legend(labels=df.columns)
plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x700 with 1 Axes>
--------------------------------------------------------------------------------
Cell 8
Cell type: code
-- Code --
n = len(df)
test_n = int(np.floor(TEST_SIZE * n))
trainval_n = n - test_n
val_n = int(np.floor(VAL_SIZE * trainval_n))
train_n = trainval_n - val_n

train_df = df.iloc[:train_n].copy()
val_df   = df.iloc[train_n:train_n + val_n].copy()
test_df  = df.iloc[train_n + val_n:].copy()

print(f"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%) | "
      f"Validation: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%) | "
      f"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")
-- Outputs --
[1] output_type: stream
Train: 2632 (72.0%) | Validation: 292 (8.0%) | Test: 730 (20.0%)
--------------------------------------------------------------------------------
Cell 9
Cell type: code
-- Code --
if INCLUDE_TARGET_AS_FEATURE:
    feature_cols = df.columns.tolist()          # includes TARGET_COL (JKSE)
else:
    feature_cols = [c for c in df.columns if c != TARGET_COL]
--------------------------------------------------------------------------------
Cell 10
Cell type: code
-- Code --
# feature_cols = [c for c in df.columns if c != TARGET_COL]
X_scaler = StandardScaler()
y_scaler = StandardScaler()
X_scaler.fit(train_df[feature_cols])
y_scaler.fit(train_df[[TARGET_COL]])

def scale_block(block):
    X = X_scaler.transform(block[feature_cols])
    y = y_scaler.transform(block[[TARGET_COL]])
    return pd.DataFrame(X, index=block.index, columns=feature_cols), pd.DataFrame(y, index=block.index, columns=[TARGET_COL])

X_train_s, y_train_s = scale_block(train_df)
X_val_s,   y_val_s   = scale_block(val_df)
X_test_s,  y_test_s  = scale_block(test_df)
--------------------------------------------------------------------------------
Cell 11
Cell type: code
-- Code --
def make_windows(X_df, y_df, lookback):
    X_vals = X_df.values
    y_vals = y_df.values.squeeze()
    idx = X_df.index

    X_list, y_list, idx_list = [], [], []
    for i in range(lookback, len(X_df)):
        X_list.append(X_vals[i - lookback:i, :])
        y_list.append(y_vals[i])
        idx_list.append(idx[i])

    X_arr = np.array(X_list, dtype=np.float32)
    y_arr = np.array(y_list, dtype=np.float32)
    idx_arr = np.array(idx_list)
    return X_arr, y_arr, idx_arr
--------------------------------------------------------------------------------
Cell 12
Cell type: code
-- Code --
def compute_metrics(y_true, y_pred):
    mse  = mean_squared_error(y_true, y_pred)
    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    return [mse, mae, rmse, mape, r2]
--------------------------------------------------------------------------------
Cell 13
Cell type: code
-- Code --
def build_lstm(trial, lookback, n_features):
    # hyperparameters
    num_layers   = trial.suggest_int("num_layers", 1, 2)
    units1       = trial.suggest_int("units1", 32, 256, step=32)
    units2       = trial.suggest_int("units2", 32, 256, step=32) if num_layers == 2 else None
    dropout      = trial.suggest_float("dropout", 0.0, 0.5)
    lr           = trial.suggest_float("lr", 1e-4, 5e-3, log=True)

    model = Sequential()
    model.add(Input(shape=(lookback, n_features)))
    if num_layers == 2:
        model.add(LSTM(units1, return_sequences=True))
        model.add(Dropout(dropout))
        model.add(LSTM(units2))
    else:
        model.add(LSTM(units1))
    model.add(Dropout(dropout))
    model.add(Dense(1))

    opt = optimizers.Adam(learning_rate=lr)
    model.compile(optimizer=opt, loss="mse")
    return model
--------------------------------------------------------------------------------
Cell 14
Cell type: code
-- Code --
def objective(trial):
    # allow lookback to change per trial; rebuild windows
    lookback = trial.suggest_categorical("lookback", [30, 45, 60, 90])

    X_tr, y_tr, _ = make_windows(X_train_s, y_train_s, lookback)
    X_va, y_va, _ = make_windows(X_val_s,   y_val_s,   lookback)

    batch_size = trial.suggest_categorical("batch_size", [32, 64, 128])
    epochs     = trial.suggest_int("epochs", 30, 100, step=10)
    patience   = trial.suggest_int("patience", 5, 10)

    model = build_lstm(trial, lookback, n_features=len(feature_cols))

    callbacks = [
        EarlyStopping(monitor="val_loss", patience=patience, restore_best_weights=True),
        TFKerasPruningCallback(trial, monitor="val_loss")
    ]

    history = model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        epochs=epochs,
        batch_size=batch_size,
        verbose=0,
        callbacks=callbacks
    )

    # Return the best validation loss for this trial
    return min(history.history["val_loss"])
--------------------------------------------------------------------------------
Cell 15
Cell type: code
-- Code --
# import random
# SAMPLER_SEED = random.getrandbits(32)
# print(SAMPLER_SEED)
print(RANDOM_SEED)
-- Outputs --
[1] output_type: stream
271828183
--------------------------------------------------------------------------------
Cell 16
Cell type: code
-- Code --
sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)
pruner  = optuna.pruners.MedianPruner(n_warmup_steps=5)
study = optuna.create_study(direction="minimize", sampler=sampler, pruner=pruner)
print("\nStarting Optuna study...")

start_opt = time.time()
study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)
end_opt = time.time()
print(f"Optuna finished in {end_opt - start_opt:.4f} seconds")
print("Best trial:", study.best_trial.number)
print("Best val_loss:", study.best_value)
print("Best params:", study.best_params)

best_params = study.best_params
BEST_LOOKBACK = best_params["lookback"]
-- Outputs --
[1] output_type: stream
[I 2025-10-28 12:36:41,690] A new study created in memory with name: no-name-2fa99482-20be-45ef-9be6-92b9e90d6b9a
[2] output_type: stream

Starting Optuna study...
[3] output_type: display_data
  0%|          | 0/50 [00:00<?, ?it/s]
[4] output_type: stream
[I 2025-10-28 12:37:26,383] Trial 0 finished with value: 0.07990353554487228 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 60, 'patience': 6, 'num_layers': 2, 'units1': 224, 'units2': 32, 'dropout': 0.27630689363195354, 'lr': 0.00011434542851382032}. Best is trial 0 with value: 0.07990353554487228.
[I 2025-10-28 12:38:07,514] Trial 1 finished with value: 0.006918891333043575 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 30, 'patience': 7, 'num_layers': 1, 'units1': 128, 'dropout': 0.44514116177523977, 'lr': 0.0044352076683962585}. Best is trial 1 with value: 0.006918891333043575.
[I 2025-10-28 12:39:17,846] Trial 2 finished with value: 0.03298492357134819 and parameters: {'lookback': 45, 'batch_size': 128, 'epochs': 50, 'patience': 8, 'num_layers': 2, 'units1': 96, 'units2': 192, 'dropout': 0.4046970776964004, 'lr': 0.0013795479406003674}. Best is trial 1 with value: 0.006918891333043575.
[I 2025-10-28 12:40:59,025] Trial 3 finished with value: 0.020514531061053276 and parameters: {'lookback': 60, 'batch_size': 128, 'epochs': 30, 'patience': 7, 'num_layers': 2, 'units1': 224, 'units2': 160, 'dropout': 0.2737757246122824, 'lr': 0.0025659160398261963}. Best is trial 1 with value: 0.006918891333043575.
[I 2025-10-28 12:42:31,196] Trial 4 finished with value: 0.01426685694605112 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 90, 'patience': 8, 'num_layers': 2, 'units1': 96, 'units2': 128, 'dropout': 0.32186983531757285, 'lr': 0.001329352768406611}. Best is trial 1 with value: 0.006918891333043575.
[I 2025-10-28 12:42:58,437] Trial 5 pruned. Trial was pruned at epoch 8.
[I 2025-10-28 12:43:21,431] Trial 6 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:43:31,916] Trial 7 pruned. Trial was pruned at epoch 8.
[I 2025-10-28 12:48:10,733] Trial 8 finished with value: 0.008125209249556065 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 40, 'patience': 7, 'num_layers': 2, 'units1': 256, 'units2': 160, 'dropout': 0.17381721991447519, 'lr': 0.0005498668057794269}. Best is trial 1 with value: 0.006918891333043575.
[I 2025-10-28 12:48:23,312] Trial 9 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:49:17,236] Trial 10 finished with value: 0.005508108995854855 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 30, 'patience': 5, 'num_layers': 1, 'units1': 160, 'dropout': 0.47966550998315194, 'lr': 0.004713710957458598}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:49:51,512] Trial 11 finished with value: 0.009456474334001541 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 30, 'patience': 5, 'num_layers': 1, 'units1': 160, 'dropout': 0.48472005782053945, 'lr': 0.0049824624618880035}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:50:17,971] Trial 12 finished with value: 0.019944308325648308 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 30, 'patience': 5, 'num_layers': 1, 'units1': 160, 'dropout': 0.4833379572340881, 'lr': 0.004162557100536866}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:50:40,996] Trial 13 pruned. Trial was pruned at epoch 8.
[I 2025-10-28 12:51:05,313] Trial 14 pruned. Trial was pruned at epoch 7.
[I 2025-10-28 12:51:23,650] Trial 15 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:52:07,322] Trial 16 finished with value: 0.007729997858405113 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 30, 'patience': 5, 'num_layers': 1, 'units1': 128, 'dropout': 0.3490727733965464, 'lr': 0.004711549348611887}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:52:13,088] Trial 17 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:52:24,268] Trial 18 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:53:31,618] Trial 19 finished with value: 0.006507713347673416 and parameters: {'lookback': 60, 'batch_size': 64, 'epochs': 80, 'patience': 5, 'num_layers': 1, 'units1': 192, 'dropout': 0.31779789865776864, 'lr': 0.003456216620105615}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:53:48,413] Trial 20 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:54:01,488] Trial 21 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:54:22,714] Trial 22 finished with value: 0.02252768911421299 and parameters: {'lookback': 60, 'batch_size': 64, 'epochs': 70, 'patience': 5, 'num_layers': 1, 'units1': 192, 'dropout': 0.36995724384494355, 'lr': 0.003581543404298459}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:54:49,010] Trial 23 pruned. Trial was pruned at epoch 8.
[I 2025-10-28 12:55:29,250] Trial 24 finished with value: 0.012341689318418503 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 50, 'patience': 5, 'num_layers': 1, 'units1': 160, 'dropout': 0.49959156460398113, 'lr': 0.002272461172813803}. Best is trial 10 with value: 0.005508108995854855.
[I 2025-10-28 12:55:43,722] Trial 25 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:55:54,490] Trial 26 pruned. Trial was pruned at epoch 7.
[I 2025-10-28 12:56:54,178] Trial 27 finished with value: 0.00430006580427289 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 60, 'patience': 9, 'num_layers': 1, 'units1': 160, 'dropout': 0.42397952138877115, 'lr': 0.004963577327041489}. Best is trial 27 with value: 0.00430006580427289.
[I 2025-10-28 12:57:03,636] Trial 28 pruned. Trial was pruned at epoch 7.
[I 2025-10-28 12:57:14,867] Trial 29 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:58:13,376] Trial 30 finished with value: 0.004649546463042498 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 70, 'patience': 10, 'num_layers': 1, 'units1': 160, 'dropout': 0.3099907016138821, 'lr': 0.0021905292565240404}. Best is trial 27 with value: 0.00430006580427289.
[I 2025-10-28 12:58:58,523] Trial 31 pruned. Trial was pruned at epoch 27.
[I 2025-10-28 12:59:54,018] Trial 32 finished with value: 0.004055649973452091 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 80, 'patience': 10, 'num_layers': 1, 'units1': 160, 'dropout': 0.41833496692617117, 'lr': 0.0049925159194487674}. Best is trial 32 with value: 0.004055649973452091.
[I 2025-10-28 13:01:02,448] Trial 33 finished with value: 0.004160185344517231 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 90, 'patience': 10, 'num_layers': 1, 'units1': 160, 'dropout': 0.43120332888155083, 'lr': 0.004783762613467998}. Best is trial 32 with value: 0.004055649973452091.
[I 2025-10-28 13:02:01,148] Trial 34 finished with value: 0.004017048981040716 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 90, 'patience': 10, 'num_layers': 1, 'units1': 128, 'dropout': 0.42239596052620226, 'lr': 0.002034202209425004}. Best is trial 34 with value: 0.004017048981040716.
[I 2025-10-28 13:02:21,777] Trial 35 pruned. Trial was pruned at epoch 15.
[I 2025-10-28 13:02:51,350] Trial 36 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 13:03:06,259] Trial 37 pruned. Trial was pruned at epoch 8.
[I 2025-10-28 13:03:41,708] Trial 38 pruned. Trial was pruned at epoch 15.
[I 2025-10-28 13:03:47,096] Trial 39 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 13:04:41,262] Trial 40 pruned. Trial was pruned at epoch 12.
[I 2025-10-28 13:05:22,899] Trial 41 finished with value: 0.006655794568359852 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 70, 'patience': 10, 'num_layers': 1, 'units1': 160, 'dropout': 0.4078784711671436, 'lr': 0.0009931812099269802}. Best is trial 34 with value: 0.004017048981040716.
[I 2025-10-28 13:06:12,674] Trial 42 pruned. Trial was pruned at epoch 32.
[I 2025-10-28 13:06:18,568] Trial 43 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 13:07:03,788] Trial 44 finished with value: 0.0036153129767626524 and parameters: {'lookback': 30, 'batch_size': 32, 'epochs': 80, 'patience': 9, 'num_layers': 1, 'units1': 160, 'dropout': 0.22279212439060975, 'lr': 0.004088701856336038}. Best is trial 44 with value: 0.0036153129767626524.
[I 2025-10-28 13:09:24,837] Trial 45 finished with value: 0.0042701237834990025 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 90, 'patience': 9, 'num_layers': 1, 'units1': 192, 'dropout': 0.15737597234905565, 'lr': 0.004091768439122648}. Best is trial 44 with value: 0.0036153129767626524.
[I 2025-10-28 13:11:59,929] Trial 46 finished with value: 0.004042791668325663 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 90, 'patience': 8, 'num_layers': 1, 'units1': 224, 'dropout': 0.1507240487367352, 'lr': 0.004021361029778722}. Best is trial 44 with value: 0.0036153129767626524.
[I 2025-10-28 13:14:36,202] Trial 47 finished with value: 0.004430172499269247 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 90, 'patience': 8, 'num_layers': 1, 'units1': 256, 'dropout': 0.11547660782932687, 'lr': 0.0033170217759703256}. Best is trial 44 with value: 0.0036153129767626524.
[I 2025-10-28 13:14:59,746] Trial 48 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 13:16:56,704] Trial 49 finished with value: 0.004770637024194002 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 90, 'patience': 9, 'num_layers': 1, 'units1': 224, 'dropout': 0.23333342013720026, 'lr': 0.004190480349509213}. Best is trial 44 with value: 0.0036153129767626524.
Optuna finished in 2415.0158 seconds
Best trial: 44
Best val_loss: 0.0036153129767626524
Best params: {'lookback': 30, 'batch_size': 32, 'epochs': 80, 'patience': 9, 'num_layers': 1, 'units1': 160, 'dropout': 0.22279212439060975, 'lr': 0.004088701856336038}
--------------------------------------------------------------------------------
Cell 17
Cell type: code
-- Code --
X_train_w, y_train_w, idx_train = make_windows(X_train_s, y_train_s, BEST_LOOKBACK)
X_val_w,   y_val_w,   idx_val   = make_windows(X_val_s,   y_val_s,   BEST_LOOKBACK)
X_test_w,  y_test_w,  idx_test  = make_windows(X_test_s,  y_test_s,  BEST_LOOKBACK)
--------------------------------------------------------------------------------
Cell 18
Cell type: code
-- Code --
# X_trval = np.concatenate([X_train_w, X_val_w], axis=0)
# y_trval = np.concatenate([y_train_w, y_val_w], axis=0)

final_model = Sequential()
final_model.add(Input(shape=(BEST_LOOKBACK, len(feature_cols))))

if best_params["num_layers"] == 2:
    final_model.add(LSTM(best_params["units1"], return_sequences=True))
    final_model.add(Dropout(best_params["dropout"]))
    final_model.add(LSTM(best_params["units2"]))
else:
    final_model.add(LSTM(best_params["units1"]))
final_model.add(Dropout(best_params["dropout"]))
final_model.add(Dense(1))

final_model.compile(
    optimizer=optimizers.Adam(learning_rate=best_params["lr"]),
    loss="mse"
)

callbacks = [
    EarlyStopping(monitor="val_loss", patience=best_params["patience"], restore_best_weights=True),
    ModelCheckpoint("lstm_optuna_best.keras", monitor="val_loss", save_best_only=True)
]

print("\nRetraining final model on Train+Val with best hyperparameters...")
start_train = time.time()
history = final_model.fit(
    X_train_w, y_train_w,
    validation_data=(X_val_w, y_val_w),
    epochs=best_params["epochs"],
    batch_size=best_params["batch_size"],
    callbacks=[
        EarlyStopping(monitor="val_loss", patience=best_params["patience"], restore_best_weights=True),
        ModelCheckpoint("Model Checkpoints/lstm_optuna_best.keras", monitor="val_loss", save_best_only=True)
    ],
    verbose=1
)
end_train = time.time()
print(f"Final training time: {end_train - start_train:.4f} seconds")
-- Outputs --
[1] output_type: stream

Retraining final model on Train+Val with best hyperparameters...
Epoch 1/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m3s[0m 20ms/step - loss: 0.1641 - val_loss: 0.0338
Epoch 2/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0257 - val_loss: 0.0163
Epoch 3/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0171 - val_loss: 0.0136
Epoch 4/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0148 - val_loss: 0.0097
Epoch 5/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0136 - val_loss: 0.0230
Epoch 6/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0127 - val_loss: 0.0135
Epoch 7/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0127 - val_loss: 0.0081
Epoch 8/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0108 - val_loss: 0.0062
Epoch 9/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0112 - val_loss: 0.0165
Epoch 10/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0113 - val_loss: 0.0107
Epoch 11/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0100 - val_loss: 0.0144
Epoch 12/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0110 - val_loss: 0.0102
Epoch 13/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0099 - val_loss: 0.0082
Epoch 14/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0103 - val_loss: 0.0303
Epoch 15/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0114 - val_loss: 0.0041
Epoch 16/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0102 - val_loss: 0.0094
Epoch 17/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0117 - val_loss: 0.0148
Epoch 18/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0092 - val_loss: 0.0047
Epoch 19/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0095 - val_loss: 0.0074
Epoch 20/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0095 - val_loss: 0.0057
Epoch 21/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0091 - val_loss: 0.0051
Epoch 22/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0084 - val_loss: 0.0067
Epoch 23/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0087 - val_loss: 0.0058
Epoch 24/80
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m1s[0m 17ms/step - loss: 0.0100 - val_loss: 0.0118
Final training time: 36.4545 seconds
--------------------------------------------------------------------------------
Cell 19
Cell type: code
-- Code --
def predict_series(model, X_block, idx_block):
    yhat_s = final_model.predict(X_block, verbose=0)
    yhat = y_scaler.inverse_transform(yhat_s).squeeze()
    return pd.Series(yhat, index=idx_block, name="Pred")

# Predictions on each split (use windows aligned to BEST_LOOKBACK)
pred_train = predict_series(final_model, X_train_w, idx_train)
pred_val   = predict_series(final_model, X_val_w,   idx_val)

start_test = time.time()
pred_test  = predict_series(final_model, X_test_w,  idx_test)
end_test = time.time()
print(f"Testing (inference) time: {end_test - start_test:.4f} seconds")
-- Outputs --
[1] output_type: stream
Testing (inference) time: 0.2001 seconds
--------------------------------------------------------------------------------
Cell 20
Cell type: code
-- Code --
actual = df[TARGET_COL]
actual_train = actual.loc[idx_train]
actual_val   = actual.loc[idx_val]
actual_test  = actual.loc[idx_test]
--------------------------------------------------------------------------------
Cell 21
Cell type: code
-- Code --
metrics_train = compute_metrics(actual_train.values, pred_train.values)
metrics_val   = compute_metrics(actual_val.values,   pred_val.values)
metrics_test  = compute_metrics(actual_test.values,  pred_test.values)

metrics_df = pd.DataFrame(
    [metrics_train, metrics_val, metrics_test],
    columns=["MSE", "MAE", "RMSE", "MAPE", "R¬≤"],
    index=["Train", "Validation", "Test"]
)

pd.set_option("display.float_format", "{:.4f}".format)
print("\n=== Metrics Summary (GRU ‚Äî Optuna best) ===")
print(metrics_df.round(4))
-- Outputs --
[1] output_type: stream

=== Metrics Summary (GRU ‚Äî Optuna best) ===
                  MSE      MAE     RMSE   MAPE     R¬≤
Train       3498.9018  43.3275  59.1515 0.0080 0.9930
Validation  2039.3456  33.5720  45.1591 0.0048 0.9248
Test       20242.0559 121.1374 142.2746 0.0169 0.8204
--------------------------------------------------------------------------------
Cell 22
Cell type: code
-- Code --
residuals = pd.Series(actual_test.values - pred_test.values, index=actual_test.index, name="Residuals")
--------------------------------------------------------------------------------
Cell 23
Cell type: code
-- Code --
# 1) All actual vs predicted
plt.figure(figsize=(12, 5))
plt.plot(actual.index, actual.values, label="Actual (JKSE)", linewidth=1)
plt.plot(pred_train.index, pred_train.values, label="Predicted (Train)", linewidth=1)
plt.plot(pred_val.index,   pred_val.values,   label="Predicted (Val)", linewidth=1)
plt.plot(pred_test.index,  pred_test.values,  label="Predicted (Test)", linewidth=1.5)
plt.title("All Actual vs. Predicted (Train/Val/Test) ‚Äî LSTM (Optuna best)")
plt.xlabel("Date"); plt.ylabel("JKSE")
plt.legend(); plt.tight_layout(); plt.show()

# 2) Actual vs predicted ‚Äî test horizon
plt.figure(figsize=(12, 5))
plt.plot(actual_test.index, actual_test.values, label="Actual (Test horizon)", linewidth=1.5)
plt.plot(pred_test.index,   pred_test.values,   label="Predicted (Test)", linewidth=1.5)
plt.title("Actual vs. Predicted ‚Äî Test Horizon (LSTM Optuna)")
plt.xlabel("Date"); plt.ylabel("JKSE")
plt.legend(); plt.tight_layout(); plt.show()

# 3) Residuals over time
plt.figure(figsize=(12, 4))
plt.plot(residuals.index, residuals.values, linewidth=1)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals Over Time (Test) ‚Äî LSTM (Optuna)")
plt.xlabel("Date"); plt.ylabel("Residual = Actual - Predicted")
plt.tight_layout(); plt.show()

# 4) Residual histogram
plt.figure(figsize=(7, 5))
plt.hist(residuals.values, bins=50, edgecolor="black", alpha=0.8)
plt.title("Residuals Histogram (Test) ‚Äî LSTM (Optuna)")
plt.xlabel("Residual"); plt.ylabel("Frequency")
plt.tight_layout(); plt.show()

# 5) Residuals vs fitted (test)
plt.figure(figsize=(7, 5))
plt.scatter(pred_test.values, residuals.values, s=10, alpha=0.6)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals vs. Fitted (Test) ‚Äî LSTM (Optuna)")
plt.xlabel("Predicted (Fitted)"); plt.ylabel("Residual")
plt.tight_layout(); plt.show()

# 6) Train vs Validation loss (during final retraining)
plt.figure(figsize=(8,5))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Training vs Validation Loss ‚Äî LSTM (Optuna final)")
plt.xlabel("Epoch"); plt.ylabel("MSE Loss"); plt.legend(); plt.tight_layout(); plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[2] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[3] output_type: display_data
<Figure size 1200x400 with 1 Axes>
[4] output_type: display_data
<Figure size 700x500 with 1 Axes>
[5] output_type: display_data
<Figure size 700x500 with 1 Axes>
[6] output_type: display_data
<Figure size 800x500 with 1 Axes>
--------------------------------------------------------------------------------
Cell 24
Cell type: code
-- Code --
# === CONFIGURATION ===
results_dir = os.path.join("..", "Results")
predicted_path = os.path.join(results_dir, "ALL_PREDICTED.csv")
metrics_path = os.path.join(results_dir, "ALL_METRICS.csv")

# Manual model name (since __file__ isn't available in notebooks)
model = "lstm_optuna"
model_name = f"{model}_{RANDOM_SEED}"   # change this for each notebook (e.g., GRU_Baseline)
print("Model Name for Documentation:", model_name)

# Create Results directory if not exists
os.makedirs(results_dir, exist_ok=True)

# ==========================================
# 1Ô∏è‚É£ PREPARE AND ALIGN TESTING DATAFRAME
# ==========================================

# Convert dates
test_dates = test_df.index.to_series().reset_index(drop=True)
actual_values = test_df[TARGET_COL].values

# If ALL_PREDICTED doesn't exist, create the base file
if not os.path.exists(predicted_path):
    print("Creating ALL_PREDICTED.csv ...")
    base_df = pd.DataFrame({
        "date": test_dates,
        "actual": actual_values
    })
    base_df.to_csv(predicted_path, index=False)

# Load and ensure datetime consistency
all_pred_df = pd.read_csv(predicted_path)
all_pred_df["date"] = pd.to_datetime(all_pred_df["date"])

# Ensure the file covers full test range (in case it was made from smaller data)
base_df = pd.DataFrame({
    "date": test_dates,
    "actual": actual_values
})
# Outer merge to make sure we have the full timeline
all_pred_df = pd.merge(base_df, all_pred_df, on=["date", "actual"], how="outer")

# Create new prediction column (aligned to date)
pred_series = pd.Series(pred_test.values, index=pd.to_datetime(idx_test), name=model_name)
pred_series = pred_series.reindex(all_pred_df["date"])  # align by date

# Add or update the model column
all_pred_df[model_name] = pred_series.values

# Sort and save
all_pred_df = all_pred_df.sort_values("date").reset_index(drop=True)
all_pred_df.to_csv(predicted_path, index=False)
print(f"‚úÖ Predictions saved to {predicted_path}")

# ==========================================
# 2Ô∏è‚É£ RECORD METRICS SUMMARY
# ==========================================
metrics_columns = [
    "timestamp",
    "model_name", 
    "seed",
    "mse",
    "mae",
    "rmse", 
    "mape",
    "r2_score",
    "picp",
    "mpiw",
    "winkler_score",
    "training_time_s",
    "testing_time_s", 
    "hpo_trial_s",
    "hpo_time_s"
]

# Create ALL_METRICS if missing
if not os.path.exists(metrics_path):
    print("Creating ALL_METRICS.csv ...")
    pd.DataFrame(columns=metrics_columns).to_csv(metrics_path, index=False)

# Current timestamp
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Extract metrics
mse, mae, rmse, mape, r2 = metrics_test
picp = mpiw = winkler = 0

# Build metrics row
metrics_row = {
    "timestamp": timestamp,
    "model_name": model_name,
    "seed": RANDOM_SEED,
    "mse": mse,
    "mae": mae,
    "rmse": rmse,
    "mape": mape,
    "r2_score": r2,
    "picp": picp,
    "mpiw": mpiw,
    "winkler_score": winkler,
    "training_time_s": round(end_train - start_train, 4),
    "testing_time_s": round(end_test - start_test, 4),
    "hpo_trial_s": N_TRIALS,
    "hpo_time_s": round(end_opt - start_opt, 4),
}

# Append metrics
all_metrics_df = pd.read_csv(metrics_path)
all_metrics_df = pd.concat([all_metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)
all_metrics_df.to_csv(metrics_path, index=False)
print(f"‚úÖ Metrics appended to {metrics_path}")

print("\nüìÑ Documentation of predictions and metrics completed successfully.")
-- Outputs --
[1] output_type: stream
Model Name for Documentation: lstm_optuna_271828183
‚úÖ Predictions saved to ..\Results\ALL_PREDICTED.csv
‚úÖ Metrics appended to ..\Results\ALL_METRICS.csv

üìÑ Documentation of predictions and metrics completed successfully.
--------------------------------------------------------------------------------
