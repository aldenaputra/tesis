Notebook: GRU_Optuna.ipynb
Kernelspec: {'display_name': 'tesis_env', 'language': 'python', 'name': 'python3'}

--------------------------------------------------------------------------------
Cell 1
Cell type: code
-- Code --
!pip install -q optuna optuna-integration[tfkeras]
-- Outputs --
[1] output_type: stream

[notice] A new release of pip is available: 23.1.2 -> 25.3
[notice] To update, run: python.exe -m pip install --upgrade pip
--------------------------------------------------------------------------------
Cell 2
Cell type: code
-- Code --
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import optuna
import random
import os

from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

import tensorflow as tf
from tensorflow.keras import Sequential, Input, optimizers
from tensorflow.keras.layers import GRU, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from optuna.integration import TFKerasPruningCallback
--------------------------------------------------------------------------------
Cell 3
Cell type: code
-- Code --
file_name = ".SEED.txt"
with open(file_name, "r") as file:
    content = file.read().strip()  # Read and remove any extra whitespace/newlines
    number = int(content)  # Use float() to support decimal; use int() if it's always an integer

print("Seed:", number)
print("Type:", type(number))
-- Outputs --
[1] output_type: stream
Seed: 271828183
Type: <class 'int'>
--------------------------------------------------------------------------------
Cell 4
Cell type: code
-- Code --
CSV_PATH   = "ALL_MERGED.csv"   # <--- set your CSV path
DATE_COL   = "Date"
TARGET_COL = "JKSE"
INCLUDE_TARGET_AS_FEATURE = True  # set False to exclude JKSE from X

TEST_SIZE  = 0.20
VAL_SIZE   = 0.10

N_TRIALS   = 50          # increase for a more thorough search
RANDOM_SEED = number

print(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)
-- Outputs --
[1] output_type: stream
271828183
--------------------------------------------------------------------------------
Cell 5
Cell type: code
-- Code --
df = pd.read_csv(CSV_PATH)
required = [
    "Date","Nickel_Fut","Coal_Fut_Newcastle","Palm_Oil_Fut",
    "USD_IDR","CNY_IDR","EUR_IDR","BTC_USD",
    "FTSE100","HANGSENG","NIKKEI225","SNP500","DOW30","SSE_Composite","JKSE"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns: {missing}")

df[DATE_COL] = pd.to_datetime(df[DATE_COL])
df = df.sort_values(DATE_COL).set_index(DATE_COL)
df = df.ffill().bfill()
df
-- Outputs --
[1] output_type: execute_result
            Nickel_Fut  Coal_Fut_Newcastle  Palm_Oil_Fut  USD_IDR  CNY_IDR  \
Date                                                                         
2015-05-01    13750.00               62.20          2136  12962.5  2089.78   
2015-05-02    13750.00               62.20          2136  12962.5  2089.78   
2015-05-03    13750.00               62.20          2136  12962.5  2089.78   
2015-05-04    13750.00               62.20          2136  12980.0  2090.51   
2015-05-05    14245.00               62.50          2136  13047.5  2102.39   
...                ...                 ...           ...      ...      ...   
2025-04-27    15482.88               93.75          4057  16830.0  2309.37   
2025-04-28    15637.63               95.60          3943  16855.0  2310.59   
2025-04-29    15564.00               97.25          3911  16760.0  2305.52   
2025-04-30    15342.50               97.50          3911  16600.0  2282.94   
2025-05-01    15218.63               97.50          3881  16600.0  2282.94   

            EUR_IDR       BTC_USD      FTSE100     HANGSENG    NIKKEI225  \
Date                                                                       
2015-05-01  14519.9    232.078995  6986.000000  28123.82031  19531.63086   
2015-05-02  14519.9    234.929993  6986.000000  28123.82031  19531.63086   
2015-05-03  14519.9    240.358002  6986.000000  28123.82031  19531.63086   
2015-05-04  14467.5    239.018005  6986.000000  28123.82031  19531.63086   
2015-05-05  14594.9    236.121002  6927.600098  27755.53906  19531.63086   
...             ...           ...          ...          ...          ...   
2025-04-27  19127.3  93754.843750  8415.299805  21980.74023  35705.73828   
2025-04-28  19251.8  94978.750000  8417.299805  21971.96094  35839.98828   
2025-04-29  19082.1  94284.789060  8463.500000  22008.10938  35839.98828   
2025-04-30  18805.3  94207.312500  8494.900391  22119.41016  36045.37891   
2025-05-01  18745.6  96492.335940  8496.799805  22119.41016  36452.30078   

                 SNP500        DOW30  SSE_Composite         JKSE  
Date                                                              
2015-05-01  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-02  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-03  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-04  2114.489990  18070.40039    4480.463867  5140.937012  
2015-05-05  2089.459961  17928.19922    4298.706055  5160.107422  
...                 ...          ...            ...          ...  
2025-04-27  5525.209961  40113.50000    3295.060059  6678.915039  
2025-04-28  5528.750000  40227.58984    3288.415039  6722.965820  
2025-04-29  5560.830078  40527.62109    3286.655029  6749.075195  
2025-04-30  5569.060059  40669.35938    3279.031006  6766.794922  
2025-05-01  5604.140137  40752.96094    3279.031006  6766.794922  

[3654 rows x 14 columns]
--------------------------------------------------------------------------------
Cell 6
Cell type: code
-- Code --
plt.figure(figsize=(12, 7))
plt.plot(df.index, df)
plt.title(f"Dataset Plot")
plt.xlabel("Date"); plt.ylabel("Price")
plt.legend(labels=df.columns)
plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x700 with 1 Axes>
--------------------------------------------------------------------------------
Cell 7
Cell type: code
-- Code --
n = len(df)
test_n = int(np.floor(TEST_SIZE * n))
trainval_n = n - test_n
val_n = int(np.floor(VAL_SIZE * trainval_n))
train_n = trainval_n - val_n

train_df = df.iloc[:train_n].copy()
val_df   = df.iloc[train_n:train_n + val_n].copy()
test_df  = df.iloc[train_n + val_n:].copy()

print(f"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%) | "
      f"Validation: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%) | "
      f"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")
-- Outputs --
[1] output_type: stream
Train: 2632 (72.0%) | Validation: 292 (8.0%) | Test: 730 (20.0%)
--------------------------------------------------------------------------------
Cell 8
Cell type: code
-- Code --
if INCLUDE_TARGET_AS_FEATURE:
    feature_cols = df.columns.tolist()          # includes TARGET_COL (JKSE)
else:
    feature_cols = [c for c in df.columns if c != TARGET_COL]
--------------------------------------------------------------------------------
Cell 9
Cell type: code
-- Code --
# feature_cols = [c for c in df.columns if c != TARGET_COL]
X_scaler = StandardScaler()
y_scaler = StandardScaler()
X_scaler.fit(train_df[feature_cols])
y_scaler.fit(train_df[[TARGET_COL]])

def scale_block(block):
    X = X_scaler.transform(block[feature_cols])
    y = y_scaler.transform(block[[TARGET_COL]])
    return pd.DataFrame(X, index=block.index, columns=feature_cols), pd.DataFrame(y, index=block.index, columns=[TARGET_COL])

X_train_s, y_train_s = scale_block(train_df)
X_val_s,   y_val_s   = scale_block(val_df)
X_test_s,  y_test_s  = scale_block(test_df)
--------------------------------------------------------------------------------
Cell 10
Cell type: code
-- Code --
def make_windows(X_df, y_df, lookback):
    X_vals = X_df.values
    y_vals = y_df.values.squeeze()
    idx = X_df.index

    X_list, y_list, idx_list = [], [], []
    for i in range(lookback, len(X_df)):
        X_list.append(X_vals[i - lookback:i, :])
        y_list.append(y_vals[i])
        idx_list.append(idx[i])

    X_arr = np.array(X_list, dtype=np.float32)
    y_arr = np.array(y_list, dtype=np.float32)
    idx_arr = np.array(idx_list)
    return X_arr, y_arr, idx_arr
--------------------------------------------------------------------------------
Cell 11
Cell type: code
-- Code --
def compute_metrics(y_true, y_pred):
    mse  = mean_squared_error(y_true, y_pred)
    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    return [mse, mae, rmse, mape, r2]
--------------------------------------------------------------------------------
Cell 12
Cell type: code
-- Code --
def build_gru(trial, lookback, n_features):
    num_layers = trial.suggest_int("num_layers", 1, 2)
    units1     = trial.suggest_int("units1", 32, 256, step=32)
    units2     = trial.suggest_int("units2", 32, 256, step=32) if num_layers == 2 else None
    dropout    = trial.suggest_float("dropout", 0.0, 0.5)
    lr         = trial.suggest_float("lr", 1e-4, 5e-3, log=True)

    model = Sequential()
    model.add(Input(shape=(lookback, n_features)))
    if num_layers == 2:
        model.add(GRU(units1, return_sequences=True))
        model.add(Dropout(dropout))
        model.add(GRU(units2))
    else:
        model.add(GRU(units1))
    model.add(Dropout(dropout))
    model.add(Dense(1))

    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss="mse")
    return model
--------------------------------------------------------------------------------
Cell 13
Cell type: code
-- Code --
def objective(trial):
    lookback = trial.suggest_categorical("lookback", [30, 45, 60, 90])
    X_tr, y_tr, _ = make_windows(X_train_s, y_train_s, lookback)
    X_va, y_va, _ = make_windows(X_val_s,   y_val_s,   lookback)

    batch_size = trial.suggest_categorical("batch_size", [32, 64, 128])
    epochs     = trial.suggest_int("epochs", 30, 100, step=10)
    patience   = trial.suggest_int("patience", 5, 10)

    model = build_gru(trial, lookback, n_features=len(feature_cols))
    callbacks = [
        EarlyStopping(monitor="val_loss", patience=patience, restore_best_weights=True),
        TFKerasPruningCallback(trial, monitor="val_loss"),
    ]

    history = model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        epochs=epochs,
        batch_size=batch_size,
        verbose=0,
        callbacks=callbacks
    )
    return min(history.history["val_loss"])
--------------------------------------------------------------------------------
Cell 14
Cell type: code
-- Code --
# import random
# SAMPLER_SEED = random.getrandbits(32)
# print(SAMPLER_SEED)
print(RANDOM_SEED)
-- Outputs --
[1] output_type: stream
271828183
--------------------------------------------------------------------------------
Cell 15
Cell type: code
-- Code --
sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)
pruner  = optuna.pruners.MedianPruner(n_warmup_steps=5)
study = optuna.create_study(direction="minimize", sampler=sampler, pruner=pruner)

print("\nStarting Optuna study...")
start_opt = time.time()
study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)
end_opt = time.time()
print(f"Optuna finished in {end_opt - start_opt:.4f} seconds")
print("Best trial:", study.best_trial.number)
print("Best val_loss:", study.best_value)
print("Best params:", study.best_params)

best_params   = study.best_params
BEST_LOOKBACK = best_params["lookback"]
-- Outputs --
[1] output_type: stream
[I 2025-10-28 10:47:13,691] A new study created in memory with name: no-name-8be15c61-3a7a-4ef7-a13e-872118593d4c
[2] output_type: stream

Starting Optuna study...
[3] output_type: display_data
  0%|          | 0/50 [00:00<?, ?it/s]
[4] output_type: stream
[I 2025-10-28 10:50:25,191] Trial 0 finished with value: 0.012794730253517628 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 60, 'patience': 6, 'num_layers': 2, 'units1': 224, 'units2': 32, 'dropout': 0.27630689363195354, 'lr': 0.00011434542851382032}. Best is trial 0 with value: 0.012794730253517628.
[I 2025-10-28 10:53:08,467] Trial 1 finished with value: 0.004259741399437189 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 30, 'patience': 7, 'num_layers': 1, 'units1': 128, 'dropout': 0.44514116177523977, 'lr': 0.0044352076683962585}. Best is trial 1 with value: 0.004259741399437189.
[I 2025-10-28 10:54:59,801] Trial 2 finished with value: 0.009671212173998356 and parameters: {'lookback': 45, 'batch_size': 128, 'epochs': 60, 'patience': 8, 'num_layers': 2, 'units1': 96, 'units2': 192, 'dropout': 0.4046970776964004, 'lr': 0.0013795479406003674}. Best is trial 1 with value: 0.004259741399437189.
[I 2025-10-28 10:57:19,352] Trial 3 finished with value: 0.006204565986990929 and parameters: {'lookback': 60, 'batch_size': 128, 'epochs': 30, 'patience': 7, 'num_layers': 2, 'units1': 224, 'units2': 160, 'dropout': 0.2737757246122824, 'lr': 0.0025659160398261963}. Best is trial 1 with value: 0.004259741399437189.
[I 2025-10-28 11:02:03,317] Trial 4 finished with value: 0.004962288308888674 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 100, 'patience': 8, 'num_layers': 2, 'units1': 96, 'units2': 128, 'dropout': 0.32186983531757285, 'lr': 0.001329352768406611}. Best is trial 1 with value: 0.004259741399437189.
[I 2025-10-28 11:03:56,146] Trial 5 pruned. Trial was pruned at epoch 13.
[I 2025-10-28 11:05:28,687] Trial 6 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:05:57,211] Trial 7 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:25:25,114] Trial 8 finished with value: 0.0034813436213880777 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 40, 'patience': 7, 'num_layers': 2, 'units1': 256, 'units2': 160, 'dropout': 0.17381721991447519, 'lr': 0.0005498668057794269}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:25:37,993] Trial 9 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:28:44,568] Trial 10 pruned. Trial was pruned at epoch 13.
[I 2025-10-28 11:30:32,843] Trial 11 finished with value: 0.0044493903405964375 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 30, 'patience': 10, 'num_layers': 1, 'units1': 160, 'dropout': 0.4649933160341135, 'lr': 0.0049824624618880035}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:32:24,647] Trial 12 pruned. Trial was pruned at epoch 27.
[I 2025-10-28 11:36:32,633] Trial 13 finished with value: 0.003984954673796892 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 50, 'patience': 6, 'num_layers': 1, 'units1': 128, 'dropout': 0.19711289259916073, 'lr': 0.0012670883005199824}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:43:07,525] Trial 14 finished with value: 0.006456824019551277 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 50, 'patience': 5, 'num_layers': 2, 'units1': 256, 'units2': 256, 'dropout': 0.20055595329801806, 'lr': 0.001317171959052326}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:43:51,991] Trial 15 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:48:47,622] Trial 16 finished with value: 0.0035477764904499054 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 50, 'patience': 6, 'num_layers': 1, 'units1': 192, 'dropout': 0.2366443375765525, 'lr': 0.0008708747986671551}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:51:08,073] Trial 17 pruned. Trial was pruned at epoch 13.
[I 2025-10-28 11:51:37,841] Trial 18 pruned. Trial was pruned at epoch 7.
[I 2025-10-28 11:52:04,130] Trial 19 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:52:40,785] Trial 20 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:53:40,379] Trial 21 finished with value: 0.004847873467952013 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 50, 'patience': 6, 'num_layers': 1, 'units1': 160, 'dropout': 0.2150224319024836, 'lr': 0.0010712051358120343}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:54:31,568] Trial 22 finished with value: 0.003745080204680562 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 50, 'patience': 6, 'num_layers': 1, 'units1': 64, 'dropout': 0.14964488027863615, 'lr': 0.0020008190132724526}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 11:54:49,004] Trial 23 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:55:06,049] Trial 24 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 11:58:29,862] Trial 25 finished with value: 0.0036932947114109993 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 60, 'patience': 6, 'num_layers': 1, 'units1': 224, 'dropout': 0.09639798541658551, 'lr': 0.0004993209546600933}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:01:42,241] Trial 26 pruned. Trial was pruned at epoch 33.
[I 2025-10-28 12:03:47,515] Trial 27 pruned. Trial was pruned at epoch 25.
[I 2025-10-28 12:05:29,570] Trial 28 pruned. Trial was pruned at epoch 33.
[I 2025-10-28 12:05:51,484] Trial 29 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:08:40,439] Trial 30 pruned. Trial was pruned at epoch 13.
[I 2025-10-28 12:10:39,144] Trial 31 finished with value: 0.00422834325581789 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 50, 'patience': 6, 'num_layers': 1, 'units1': 256, 'dropout': 0.1727195710272413, 'lr': 0.003458887965950447}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:14:35,974] Trial 32 finished with value: 0.0047452086582779884 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 60, 'patience': 7, 'num_layers': 1, 'units1': 192, 'dropout': 0.13669917170548004, 'lr': 0.001723964358149324}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:18:11,011] Trial 33 finished with value: 0.004531059879809618 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 30, 'patience': 7, 'num_layers': 1, 'units1': 224, 'dropout': 0.1631713353975679, 'lr': 0.0008813455102268408}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:18:28,348] Trial 34 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:19:09,533] Trial 35 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 12:19:41,727] Trial 36 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 12:20:13,858] Trial 37 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:21:04,818] Trial 38 finished with value: 0.0036870206240564585 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 70, 'patience': 6, 'num_layers': 1, 'units1': 192, 'dropout': 0.05613799858304448, 'lr': 0.0007644347000691739}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:21:35,590] Trial 39 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 12:23:27,957] Trial 40 finished with value: 0.003912605810910463 and parameters: {'lookback': 60, 'batch_size': 32, 'epochs': 80, 'patience': 5, 'num_layers': 1, 'units1': 224, 'dropout': 0.009113458608704106, 'lr': 0.0010264583729304663}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:23:44,746] Trial 41 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:24:16,218] Trial 42 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:24:32,754] Trial 43 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:24:45,370] Trial 44 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:25:12,679] Trial 45 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 12:28:49,758] Trial 46 finished with value: 0.0042021553963422775 and parameters: {'lookback': 90, 'batch_size': 32, 'epochs': 90, 'patience': 6, 'num_layers': 1, 'units1': 256, 'dropout': 0.17884708479480718, 'lr': 0.0027395713379496146}. Best is trial 8 with value: 0.0034813436213880777.
[I 2025-10-28 12:29:22,951] Trial 47 pruned. Trial was pruned at epoch 5.
[I 2025-10-28 12:30:15,264] Trial 48 pruned. Trial was pruned at epoch 6.
[I 2025-10-28 12:30:36,297] Trial 49 pruned. Trial was pruned at epoch 5.
Optuna finished in 6202.6113 seconds
Best trial: 8
Best val_loss: 0.0034813436213880777
Best params: {'lookback': 90, 'batch_size': 32, 'epochs': 40, 'patience': 7, 'num_layers': 2, 'units1': 256, 'units2': 160, 'dropout': 0.17381721991447519, 'lr': 0.0005498668057794269}
--------------------------------------------------------------------------------
Cell 16
Cell type: code
-- Code --
X_train_w, y_train_w, idx_train = make_windows(X_train_s, y_train_s, BEST_LOOKBACK)
X_val_w,   y_val_w,   idx_val   = make_windows(X_val_s,   y_val_s,   BEST_LOOKBACK)
X_test_w,  y_test_w,  idx_test  = make_windows(X_test_s,  y_test_s,  BEST_LOOKBACK)
--------------------------------------------------------------------------------
Cell 17
Cell type: code
-- Code --
final_model = Sequential()
final_model.add(Input(shape=(BEST_LOOKBACK, len(feature_cols))))

if best_params["num_layers"] == 2:
    final_model.add(GRU(best_params["units1"], return_sequences=True))
    final_model.add(Dropout(best_params["dropout"]))
    final_model.add(GRU(best_params["units2"]))
else:
    final_model.add(GRU(best_params["units1"]))
final_model.add(Dropout(best_params["dropout"]))
final_model.add(Dense(1))

final_model.compile(optimizer=optimizers.Adam(learning_rate=best_params["lr"]), loss="mse")

callbacks = [
    EarlyStopping(monitor="val_loss", patience=best_params["patience"], restore_best_weights=True),
    ModelCheckpoint("Model Checkpoints/gru_optuna_best.keras", monitor="val_loss", save_best_only=True)
]

print("\nRetraining final GRU on TRAIN (validate on VAL)...")
start_train = time.time()
history = final_model.fit(
    X_train_w, y_train_w,
    validation_data=(X_val_w, y_val_w),
    epochs=best_params["epochs"],
    batch_size=best_params["batch_size"],
    verbose=1,
    callbacks=callbacks
)
end_train = time.time()
print(f"Final training time: {end_train - start_train:.4f} seconds")
-- Outputs --
[1] output_type: stream

Retraining final GRU on TRAIN (validate on VAL)...
Epoch 1/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m23s[0m 206ms/step - loss: 0.1572 - val_loss: 0.0135
Epoch 2/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m16s[0m 195ms/step - loss: 0.0222 - val_loss: 0.0081
Epoch 3/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m16s[0m 194ms/step - loss: 0.0197 - val_loss: 0.0086
Epoch 4/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m15s[0m 187ms/step - loss: 0.0156 - val_loss: 0.0183
Epoch 5/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m12s[0m 145ms/step - loss: 0.0143 - val_loss: 0.0104
Epoch 6/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 168ms/step - loss: 0.0141 - val_loss: 0.0224
Epoch 7/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m14s[0m 169ms/step - loss: 0.0131 - val_loss: 0.0115
Epoch 8/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m14s[0m 171ms/step - loss: 0.0122 - val_loss: 0.0131
Epoch 9/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m14s[0m 171ms/step - loss: 0.0137 - val_loss: 0.0069
Epoch 10/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m14s[0m 168ms/step - loss: 0.0106 - val_loss: 0.0102
Epoch 11/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m14s[0m 170ms/step - loss: 0.0109 - val_loss: 0.0058
Epoch 12/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m14s[0m 170ms/step - loss: 0.0102 - val_loss: 0.0100
Epoch 13/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m11s[0m 135ms/step - loss: 0.0108 - val_loss: 0.0042
Epoch 14/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 159ms/step - loss: 0.0113 - val_loss: 0.0097
Epoch 15/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 164ms/step - loss: 0.0109 - val_loss: 0.0083
Epoch 16/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 162ms/step - loss: 0.0103 - val_loss: 0.0043
Epoch 17/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 164ms/step - loss: 0.0096 - val_loss: 0.0070
Epoch 18/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 163ms/step - loss: 0.0100 - val_loss: 0.0156
Epoch 19/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 163ms/step - loss: 0.0092 - val_loss: 0.0045
Epoch 20/40
[1m80/80[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m13s[0m 163ms/step - loss: 0.0097 - val_loss: 0.0043
Final training time: 278.7785 seconds
--------------------------------------------------------------------------------
Cell 18
Cell type: code
-- Code --
def predict_series(model, X_block, idx_block):
    yhat_s = model.predict(X_block, verbose=0)
    yhat = y_scaler.inverse_transform(yhat_s).squeeze()
    return pd.Series(yhat, index=idx_block, name="Pred")

pred_train = predict_series(final_model, X_train_w, idx_train)
pred_val   = predict_series(final_model, X_val_w,   idx_val)

start_test = time.time()
pred_test  = predict_series(final_model, X_test_w,  idx_test)
end_test = time.time()
print(f"Testing (inference) time: {end_test - start_test:.4f} seconds")
-- Outputs --
[1] output_type: stream
Testing (inference) time: 1.2461 seconds
--------------------------------------------------------------------------------
Cell 19
Cell type: code
-- Code --
actual       = df[TARGET_COL]
actual_train = actual.loc[idx_train]
actual_val   = actual.loc[idx_val]
actual_test  = actual.loc[idx_test]
--------------------------------------------------------------------------------
Cell 20
Cell type: code
-- Code --
metrics_train = compute_metrics(actual_train.values, pred_train.values)
metrics_val   = compute_metrics(actual_val.values,   pred_val.values)
metrics_test  = compute_metrics(actual_test.values,  pred_test.values)

metrics_df = pd.DataFrame(
    [metrics_train, metrics_val, metrics_test],
    columns=["MSE", "MAE", "RMSE", "MAPE", "R¬≤"],
    index=["Train", "Validation", "Test"]
)

pd.set_option("display.float_format", "{:.4f}".format)
print("\n=== Metrics Summary (GRU ‚Äî Optuna best) ===")
print(metrics_df.round(4))
-- Outputs --
[1] output_type: stream

=== Metrics Summary (GRU ‚Äî Optuna best) ===
                 MSE     MAE    RMSE   MAPE     R¬≤
Train      2990.0025 39.7240 54.6809 0.0070 0.9940
Validation 2109.8921 33.7376 45.9336 0.0049 0.8610
Test       7899.7227 67.9084 88.8804 0.0095 0.9290
--------------------------------------------------------------------------------
Cell 21
Cell type: code
-- Code --
residuals = pd.Series(actual_test.values - pred_test.values, index=actual_test.index, name="Residuals")
--------------------------------------------------------------------------------
Cell 22
Cell type: code
-- Code --
# 1) All actual vs predicted
plt.figure(figsize=(12, 5))
plt.plot(actual.index, actual.values, label="Actual (JKSE)", linewidth=1)
plt.plot(pred_train.index, pred_train.values, label="Predicted (Train)", linewidth=1)
plt.plot(pred_val.index,   pred_val.values,   label="Predicted (Val)", linewidth=1)
plt.plot(pred_test.index,  pred_test.values,  label="Predicted (Test)", linewidth=1.5)
plt.title("All Actual vs. Predicted (Train/Val/Test) ‚Äî GRU (Optuna best)")
plt.xlabel("Date"); plt.ylabel("JKSE")
plt.legend(); plt.tight_layout(); plt.show()

# 2) Actual vs predicted ‚Äî test horizon
plt.figure(figsize=(12, 5))
plt.plot(actual_test.index, actual_test.values, label="Actual (Test horizon)", linewidth=1.5)
plt.plot(pred_test.index,   pred_test.values,   label="Predicted (Test)", linewidth=1.5)
plt.title("Actual vs. Predicted ‚Äî Test Horizon (GRU Optuna)")
plt.xlabel("Date"); plt.ylabel("JKSE")
plt.legend(); plt.tight_layout(); plt.show()

# 3) Residuals over time
plt.figure(figsize=(12, 4))
plt.plot(residuals.index, residuals.values, linewidth=1)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals Over Time (Test) ‚Äî GRU (Optuna)")
plt.xlabel("Date"); plt.ylabel("Residual = Actual - Predicted")
plt.tight_layout(); plt.show()

# 4) Residual histogram
plt.figure(figsize=(7, 5))
plt.hist(residuals.values, bins=50, edgecolor="black", alpha=0.8)
plt.title("Residuals Histogram (Test) ‚Äî GRU (Optuna)")
plt.xlabel("Residual"); plt.ylabel("Frequency")
plt.tight_layout(); plt.show()

# 5) Residuals vs fitted (test)
plt.figure(figsize=(7, 5))
plt.scatter(pred_test.values, residuals.values, s=10, alpha=0.6)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals vs. Fitted (Test) ‚Äî GRU (Optuna)")
plt.xlabel("Predicted (Fitted)"); plt.ylabel("Residual")
plt.tight_layout(); plt.show()

# 6) Train vs Validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Training vs Validation Loss ‚Äî GRU (Optuna final)")
plt.xlabel("Epoch"); plt.ylabel("MSE Loss")
plt.legend(); plt.tight_layout(); plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[2] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[3] output_type: display_data
<Figure size 1200x400 with 1 Axes>
[4] output_type: display_data
<Figure size 700x500 with 1 Axes>
[5] output_type: display_data
<Figure size 700x500 with 1 Axes>
[6] output_type: display_data
<Figure size 800x500 with 1 Axes>
--------------------------------------------------------------------------------
Cell 23
Cell type: code
-- Code --
# === CONFIGURATION ===
results_dir = os.path.join("..", "Results")
predicted_path = os.path.join(results_dir, "ALL_PREDICTED.csv")
metrics_path = os.path.join(results_dir, "ALL_METRICS.csv")

# Manual model name (since __file__ isn't available in notebooks)
model = "gru_optuna"
model_name = f"{model}_{RANDOM_SEED}"   # change this for each notebook (e.g., GRU_Baseline)
print("Model Name for Documentation:", model_name)

# Create Results directory if not exists
os.makedirs(results_dir, exist_ok=True)

# ==========================================
# 1Ô∏è‚É£ PREPARE AND ALIGN TESTING DATAFRAME
# ==========================================

# Convert dates
test_dates = test_df.index.to_series().reset_index(drop=True)
actual_values = test_df[TARGET_COL].values

# If ALL_PREDICTED doesn't exist, create the base file
if not os.path.exists(predicted_path):
    print("Creating ALL_PREDICTED.csv ...")
    base_df = pd.DataFrame({
        "date": test_dates,
        "actual": actual_values
    })
    base_df.to_csv(predicted_path, index=False)

# Load and ensure datetime consistency
all_pred_df = pd.read_csv(predicted_path)
all_pred_df["date"] = pd.to_datetime(all_pred_df["date"])

# Ensure the file covers full test range (in case it was made from smaller data)
base_df = pd.DataFrame({
    "date": test_dates,
    "actual": actual_values
})
# Outer merge to make sure we have the full timeline
all_pred_df = pd.merge(base_df, all_pred_df, on=["date", "actual"], how="outer")

# Create new prediction column (aligned to date)
pred_series = pd.Series(pred_test.values, index=pd.to_datetime(idx_test), name=model_name)
pred_series = pred_series.reindex(all_pred_df["date"])  # align by date

# Add or update the model column
all_pred_df[model_name] = pred_series.values

# Sort and save
all_pred_df = all_pred_df.sort_values("date").reset_index(drop=True)
all_pred_df.to_csv(predicted_path, index=False)
print(f"‚úÖ Predictions saved to {predicted_path}")

# ==========================================
# 2Ô∏è‚É£ RECORD METRICS SUMMARY
# ==========================================
metrics_columns = [
    "timestamp",
    "model_name", 
    "seed",
    "mse",
    "mae",
    "rmse", 
    "mape",
    "r2_score",
    "picp",
    "mpiw",
    "winkler_score",
    "training_time_s",
    "testing_time_s", 
    "hpo_trial_s",
    "hpo_time_s"
]

# Create ALL_METRICS if missing
if not os.path.exists(metrics_path):
    print("Creating ALL_METRICS.csv ...")
    pd.DataFrame(columns=metrics_columns).to_csv(metrics_path, index=False)

# Current timestamp
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Extract metrics
mse, mae, rmse, mape, r2 = metrics_test
picp = mpiw = winkler = 0

# Build metrics row
metrics_row = {
    "timestamp": timestamp,
    "model_name": model_name,
    "seed": RANDOM_SEED,
    "mse": mse,
    "mae": mae,
    "rmse": rmse,
    "mape": mape,
    "r2_score": r2,
    "picp": picp,
    "mpiw": mpiw,
    "winkler_score": winkler,
    "training_time_s": round(end_train - start_train, 4),
    "testing_time_s": round(end_test - start_test, 4),
    "hpo_trial_s": N_TRIALS,
    "hpo_time_s": round(end_opt - start_opt, 4),
}

# Append metrics
all_metrics_df = pd.read_csv(metrics_path)
all_metrics_df = pd.concat([all_metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)
all_metrics_df.to_csv(metrics_path, index=False)
print(f"‚úÖ Metrics appended to {metrics_path}")

print("\nüìÑ Documentation of predictions and metrics completed successfully.")
-- Outputs --
[1] output_type: stream
Model Name for Documentation: gru_optuna_271828183
‚úÖ Predictions saved to ..\Results\ALL_PREDICTED.csv
‚úÖ Metrics appended to ..\Results\ALL_METRICS.csv

üìÑ Documentation of predictions and metrics completed successfully.
--------------------------------------------------------------------------------
