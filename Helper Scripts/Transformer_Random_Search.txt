Notebook: Transformer_Random_Search.ipynb
Kernelspec: {'display_name': 'tesis_env', 'language': 'python', 'name': 'python3'}

--------------------------------------------------------------------------------
Cell 1
Cell type: code
-- Code --
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time, json, random, math, os, sys

from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

import tensorflow as tf
from tensorflow.keras import Model, Input, regularizers
from tensorflow.keras.layers import (
    Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Lambda
)
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.optimizers import Adam, RMSprop
--------------------------------------------------------------------------------
Cell 2
Cell type: code
-- Code --
file_name = ".SEED.txt"
with open(file_name, "r") as file:
    content = file.read().strip()  # Read and remove any extra whitespace/newlines
    number = int(content)  # Use float() to support decimal; use int() if it's always an integer

print("Seed:", number)
print("Type:", type(number))
-- Outputs --
[1] output_type: stream
Seed: 271828183
Type: <class 'int'>
--------------------------------------------------------------------------------
Cell 3
Cell type: code
-- Code --
# ---------------- User knobs ----------------
CSV_PATH   = "ALL_MERGED.csv"   # <--- set your CSV path
DATE_COL   = "Date"
TARGET_COL = "JKSE"
INCLUDE_TARGET_AS_FEATURE = True  # False to exclude TARGET from X
TEST_SIZE   = 0.20
VAL_SIZE    = 0.10
RANDOM_SEED = number
MAX_EPOCHS  = 100
VERBOSE_TRAIN = 1

# Random search settings
N_TRIALS   = 50                # bump to 40â€“60 for deeper search
PATIENCE   = 10
WEIGHTS_BEST = "Model Weights/rs_transformer_best.weights.h5"
MODEL_BEST   = "Model Checkpoints/transformer_rs_checkpoint.keras"

print(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)
-- Outputs --
[1] output_type: stream
271828183
--------------------------------------------------------------------------------
Cell 4
Cell type: code
-- Code --
df = pd.read_csv(CSV_PATH)

required = [
    "Date","Nickel_Fut","Coal_Fut_Newcastle","Palm_Oil_Fut",
    "USD_IDR","CNY_IDR","EUR_IDR","BTC_USD",
    "FTSE100","HANGSENG","NIKKEI225","SNP500","DOW30","SSE_Composite","JKSE"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns: {missing}")

df[DATE_COL] = pd.to_datetime(df[DATE_COL])
df = df.sort_values(DATE_COL).set_index(DATE_COL)
df = df.ffill().bfill()
df
-- Outputs --
[1] output_type: execute_result
            Nickel_Fut  Coal_Fut_Newcastle  Palm_Oil_Fut  USD_IDR  CNY_IDR  \
Date                                                                         
2015-05-01    13750.00               62.20          2136  12962.5  2089.78   
2015-05-02    13750.00               62.20          2136  12962.5  2089.78   
2015-05-03    13750.00               62.20          2136  12962.5  2089.78   
2015-05-04    13750.00               62.20          2136  12980.0  2090.51   
2015-05-05    14245.00               62.50          2136  13047.5  2102.39   
...                ...                 ...           ...      ...      ...   
2025-04-27    15482.88               93.75          4057  16830.0  2309.37   
2025-04-28    15637.63               95.60          3943  16855.0  2310.59   
2025-04-29    15564.00               97.25          3911  16760.0  2305.52   
2025-04-30    15342.50               97.50          3911  16600.0  2282.94   
2025-05-01    15218.63               97.50          3881  16600.0  2282.94   

            EUR_IDR       BTC_USD      FTSE100     HANGSENG    NIKKEI225  \
Date                                                                       
2015-05-01  14519.9    232.078995  6986.000000  28123.82031  19531.63086   
2015-05-02  14519.9    234.929993  6986.000000  28123.82031  19531.63086   
2015-05-03  14519.9    240.358002  6986.000000  28123.82031  19531.63086   
2015-05-04  14467.5    239.018005  6986.000000  28123.82031  19531.63086   
2015-05-05  14594.9    236.121002  6927.600098  27755.53906  19531.63086   
...             ...           ...          ...          ...          ...   
2025-04-27  19127.3  93754.843750  8415.299805  21980.74023  35705.73828   
2025-04-28  19251.8  94978.750000  8417.299805  21971.96094  35839.98828   
2025-04-29  19082.1  94284.789060  8463.500000  22008.10938  35839.98828   
2025-04-30  18805.3  94207.312500  8494.900391  22119.41016  36045.37891   
2025-05-01  18745.6  96492.335940  8496.799805  22119.41016  36452.30078   

                 SNP500        DOW30  SSE_Composite         JKSE  
Date                                                              
2015-05-01  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-02  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-03  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-04  2114.489990  18070.40039    4480.463867  5140.937012  
2015-05-05  2089.459961  17928.19922    4298.706055  5160.107422  
...                 ...          ...            ...          ...  
2025-04-27  5525.209961  40113.50000    3295.060059  6678.915039  
2025-04-28  5528.750000  40227.58984    3288.415039  6722.965820  
2025-04-29  5560.830078  40527.62109    3286.655029  6749.075195  
2025-04-30  5569.060059  40669.35938    3279.031006  6766.794922  
2025-05-01  5604.140137  40752.96094    3279.031006  6766.794922  

[3654 rows x 14 columns]
--------------------------------------------------------------------------------
Cell 5
Cell type: code
-- Code --
plt.figure(figsize=(12, 7))
plt.plot(df.index, df)
plt.title(f"Dataset Plot")
plt.xlabel("Date"); plt.ylabel("Price")
plt.legend(labels=df.columns)
plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x700 with 1 Axes>
--------------------------------------------------------------------------------
Cell 6
Cell type: code
-- Code --
# ---------------- Split ----------------
n = len(df)
test_n = int(np.floor(TEST_SIZE * n))
trainval_n = n - test_n
val_n = int(np.floor(VAL_SIZE * trainval_n))
train_n = trainval_n - val_n

train_df = df.iloc[:train_n].copy()
val_df   = df.iloc[train_n:train_n + val_n].copy()
test_df  = df.iloc[train_n + val_n:].copy()

print(f"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%) | "
      f"Validation: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%) | "
      f"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")
-- Outputs --
[1] output_type: stream
Train: 2632 (72.0%) | Validation: 292 (8.0%) | Test: 730 (20.0%)
--------------------------------------------------------------------------------
Cell 7
Cell type: code
-- Code --
if INCLUDE_TARGET_AS_FEATURE:
    feature_cols = df.columns.tolist()
else:
    feature_cols = [c for c in df.columns if c != TARGET_COL]
feature_cols
-- Outputs --
[1] output_type: execute_result
['Nickel_Fut',
 'Coal_Fut_Newcastle',
 'Palm_Oil_Fut',
 'USD_IDR',
 'CNY_IDR',
 'EUR_IDR',
 'BTC_USD',
 'FTSE100',
 'HANGSENG',
 'NIKKEI225',
 'SNP500',
 'DOW30',
 'SSE_Composite',
 'JKSE']
--------------------------------------------------------------------------------
Cell 8
Cell type: code
-- Code --
X_scaler = StandardScaler()
y_scaler = StandardScaler()
X_scaler.fit(train_df[feature_cols])
y_scaler.fit(train_df[[TARGET_COL]])

def scale_block(block):
    X = X_scaler.transform(block[feature_cols])
    y = y_scaler.transform(block[[TARGET_COL]])
    return pd.DataFrame(X, index=block.index, columns=feature_cols), pd.DataFrame(y, index=block.index, columns=[TARGET_COL])

X_train_s, y_train_s = scale_block(train_df)
X_val_s,   y_val_s   = scale_block(val_df)
X_test_s,  y_test_s  = scale_block(test_df)
--------------------------------------------------------------------------------
Cell 9
Cell type: code
-- Code --
# ---------------- Utilities ----------------
def make_windows(X_df, y_df, lookback: int):
    X_vals = X_df.values
    y_vals = y_df.values.squeeze()
    idx = X_df.index
    X_list, y_list, idx_list = [], [], []
    for i in range(lookback, len(X_df)):
        X_list.append(X_vals[i - lookback:i, :])
        y_list.append(y_vals[i])  # predict t using t-lookback..t-1
        idx_list.append(idx[i])
    X_arr = np.array(X_list, dtype=np.float32)
    y_arr = np.array(y_list, dtype=np.float32)
    idx_arr = np.array(idx_list)
    return X_arr, y_arr, idx_arr

def positional_encoding(length, depth):
    # depth must be even for sin/cos pairing
    if depth % 2 != 0: depth += 1
    positions = np.arange(length)[:, np.newaxis]
    dims = np.arange(depth)[np.newaxis, :]
    angle_rates = 1.0 / (10000 ** (2 * (dims//2) / depth))
    angle_rads = positions * angle_rates
    pe = np.zeros((length, depth), dtype=np.float32)
    pe[:, 0::2] = np.sin(angle_rads[:, 0::2])
    pe[:, 1::2] = np.cos(angle_rads[:, 1::2])
    return tf.constant(pe)

class AddPE(tf.keras.layers.Layer):
    def __init__(self, lookback, d_model, **kwargs):
        super().__init__(**kwargs)
        self.lookback = lookback
        self.d_model = d_model
        self.pos = positional_encoding(lookback, d_model)
    def call(self, x):
        return x + self.pos

def encoder_block(x, num_heads, d_model, dff, dropout_rate):
    attn = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(
        x, x, use_causal_mask=True
    )
    x = Add()([x, Dropout(dropout_rate)(attn)])
    x = LayerNormalization(epsilon=1e-6)(x)

    ff = Dense(dff, activation="relu")(x)
    ff = Dropout(dropout_rate)(ff)
    ff = Dense(d_model)(ff)
    x = Add()([x, Dropout(dropout_rate)(ff)])
    x = LayerNormalization(epsilon=1e-6)(x)
    return x

def build_transformer(input_shape, params):
    lb = params["lookback"]
    d_model = params["d_model"]
    num_heads = params["num_heads"]
    dff = params["dff"]
    dropout = params["dropout"]

    inp = Input(shape=input_shape)
    x = Dense(d_model)(inp)
    x = AddPE(lb, d_model)(x)

    for _ in range(params["num_layers"]):
        x = encoder_block(
            x,
            num_heads=num_heads,
            d_model=d_model,
            dff=dff,
            dropout_rate=dropout
        )

    x = Lambda(lambda t: t[:, -1, :])(x)
    out = Dense(1)(x)

    model = Model(inputs=inp, outputs=out)

    # Optuna always uses Adam
    opt = Adam(learning_rate=params["lr"])
    model.compile(optimizer=opt, loss="mse")

    return model

# Make sure d_model is divisible by num_heads
def sample_divisible_pair(d_model_choices, head_choices):
    for _ in range(50):
        dm = random.choice(d_model_choices)
        nh = random.choice(head_choices)
        if dm % nh == 0:
            return dm, nh
    # fallback
    return 64, 4

def sample_params():
    lookback = random.choice([30, 45, 60, 90])
    d_model = random.choice([32, 64, 96, 128])
    valid_heads = [h for h in (2, 4, 8) if d_model % h == 0 and d_model // h >= 8]
    num_heads = random.choice(valid_heads)
    dff = random.choice([2*d_model, 3*d_model, 4*d_model])

    params = {
        "lookback": lookback,
        "d_model": d_model,
        "num_heads": num_heads,
        "dff": dff,
        "num_layers": random.choice([1, 2, 3]),
        "dropout": np.random.uniform(0.0, 0.3),
        "optimizer": "adam",
        "lr": 10 ** np.random.uniform(-4, math.log10(5e-3)),
        "batch_size": random.choice([32, 64, 128]),
        "epochs": random.choice([30, 40, 50, 60, 70, 80, 90, 100]),
        "patience": random.choice([5, 6, 7, 8, 9, 10])
    }

    return params

def compute_metrics(y_true, y_pred):
    mse  = mean_squared_error(y_true, y_pred)
    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    return [mse, mae, rmse, mape, r2]
--------------------------------------------------------------------------------
Cell 10
Cell type: code
-- Code --
# ---------------- Random Search ----------------
best = {"val_loss": np.inf, "params": None}

print("\n=== Random Search (Transformer) begins ===")
hpo_start = time.time()  # HPO timer starts
for t in range(1, N_TRIALS + 1):
    params = sample_params()

    # Build windows with this trial's lookback
    X_tr, y_tr, _ = make_windows(X_train_s, y_train_s, params["lookback"])
    X_vl, y_vl, _ = make_windows(X_val_s,   y_val_s,   params["lookback"])

    model = build_transformer((params["lookback"], len(feature_cols)), params)

    callbacks = [
        EarlyStopping(monitor="val_loss", patience=PATIENCE, restore_best_weights=True),
    ]

    start = time.time()
    hist = model.fit(
        X_tr, y_tr,
        validation_data=(X_vl, y_vl),
        epochs=params["epochs"],
        batch_size=params["batch_size"],
        callbacks=callbacks,
        verbose=0
    )
    dur = time.time() - start

    val_loss = float(min(hist.history["val_loss"]))
    print(f"[Trial {t:02d}] lb={params['lookback']}, layers={params['num_layers']}, "
          f"d_model={params['d_model']}, heads={params['num_heads']}, dff={params['dff']}, "
          f"drop={params['dropout']}, opt={params['optimizer']}, lr={params['lr']}, "
          f"bs={params['batch_size']}  => val_loss={val_loss:.6f} ({dur:.1f}s)")

    if val_loss < best["val_loss"]:
        best = {"val_loss": val_loss, "params": params}
        model.save_weights(WEIGHTS_BEST)
hpo_end = time.time()  # HPO timer ends
hpo_time_s = float(hpo_end - hpo_start)
hpo_trials = N_TRIALS

print("\nBest val_loss:", best["val_loss"])
print("Best params:\n", json.dumps(best["params"], indent=2))
-- Outputs --
[1] output_type: stream

=== Random Search (Transformer) begins ===
WARNING:tensorflow:From C:\Users\Alden\AppData\Roaming\Python\Python311\site-packages\keras\src\backend\tensorflow\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

[Trial 01] lb=90, layers=3, d_model=32, heads=2, dff=128, drop=0.1716649385278118, opt=adam, lr=0.0006023757744019968, bs=32  => val_loss=0.026598 (138.9s)
[Trial 02] lb=60, layers=1, d_model=128, heads=4, dff=384, drop=0.2870528249083718, opt=adam, lr=0.000803924347200877, bs=64  => val_loss=0.013613 (28.4s)
[Trial 03] lb=30, layers=2, d_model=32, heads=2, dff=128, drop=0.25659499640621103, opt=adam, lr=0.001776050814586662, bs=32  => val_loss=0.016288 (36.9s)
[Trial 04] lb=90, layers=3, d_model=128, heads=4, dff=512, drop=0.02350016306073822, opt=adam, lr=0.0006585208640936544, bs=128  => val_loss=0.009563 (456.7s)
[Trial 05] lb=90, layers=3, d_model=128, heads=4, dff=256, drop=0.05065361246887718, opt=adam, lr=0.0023239105457533603, bs=32  => val_loss=0.008701 (139.7s)
[Trial 06] lb=45, layers=3, d_model=32, heads=2, dff=128, drop=0.25482016014943787, opt=adam, lr=0.00015184844950893856, bs=128  => val_loss=0.020124 (41.3s)
[Trial 07] lb=30, layers=1, d_model=128, heads=8, dff=256, drop=0.16578413617917212, opt=adam, lr=0.00011434542851382034, bs=32  => val_loss=0.015183 (19.1s)
[Trial 08] lb=30, layers=3, d_model=32, heads=4, dff=128, drop=0.22152675216648396, opt=adam, lr=0.0007751660436443055, bs=64  => val_loss=0.028008 (59.0s)
[Trial 09] lb=60, layers=2, d_model=128, heads=8, dff=384, drop=0.272448806671595, opt=adam, lr=0.00017182727403434575, bs=128  => val_loss=0.025157 (212.3s)
[Trial 10] lb=60, layers=3, d_model=96, heads=8, dff=288, drop=0.09275679154962525, opt=adam, lr=0.0001170963244009816, bs=64  => val_loss=0.052792 (220.4s)
[Trial 11] lb=90, layers=1, d_model=128, heads=8, dff=256, drop=0.021443614041778112, opt=adam, lr=0.0001265580670945192, bs=64  => val_loss=0.026450 (248.1s)
[Trial 12] lb=90, layers=1, d_model=96, heads=2, dff=384, drop=0.1147490562749201, opt=adam, lr=0.0003229917593899363, bs=32  => val_loss=0.013142 (62.1s)
[Trial 13] lb=30, layers=2, d_model=128, heads=8, dff=384, drop=0.14751355228197627, opt=adam, lr=0.0032550897168514017, bs=32  => val_loss=0.005114 (73.0s)
[Trial 14] lb=90, layers=3, d_model=32, heads=4, dff=128, drop=0.29080806993897235, opt=adam, lr=0.0006102996757200963, bs=64  => val_loss=0.033741 (42.5s)
[Trial 15] lb=60, layers=2, d_model=96, heads=4, dff=288, drop=0.2870530380710973, opt=adam, lr=0.00017615938172876292, bs=64  => val_loss=0.022587 (46.7s)
[Trial 16] lb=30, layers=3, d_model=128, heads=4, dff=384, drop=0.13304215348214035, opt=adam, lr=0.0018931836123193034, bs=64  => val_loss=0.004397 (78.6s)
[Trial 17] lb=45, layers=3, d_model=96, heads=2, dff=384, drop=0.08379714834249066, opt=adam, lr=0.001958857140629401, bs=128  => val_loss=0.044200 (49.1s)
[Trial 18] lb=30, layers=2, d_model=128, heads=4, dff=384, drop=0.12237811802149506, opt=adam, lr=0.0009848339937992465, bs=32  => val_loss=0.007047 (71.3s)
[Trial 19] lb=30, layers=3, d_model=128, heads=2, dff=256, drop=0.15282611922146372, opt=adam, lr=0.0004030307529260446, bs=128  => val_loss=0.004045 (174.2s)
[Trial 20] lb=60, layers=2, d_model=64, heads=2, dff=192, drop=0.1940070412464087, opt=adam, lr=0.0023721185608034778, bs=128  => val_loss=0.013547 (36.5s)
[Trial 21] lb=45, layers=3, d_model=128, heads=4, dff=256, drop=0.20125195753558975, opt=adam, lr=0.0003374596708520818, bs=128  => val_loss=0.020063 (103.3s)
[Trial 22] lb=45, layers=1, d_model=96, heads=2, dff=288, drop=0.07768157722419032, opt=adam, lr=0.0027449436065164834, bs=64  => val_loss=0.007391 (30.3s)
[Trial 23] lb=45, layers=2, d_model=32, heads=2, dff=64, drop=0.1480880221014329, opt=adam, lr=0.00037831608787368333, bs=64  => val_loss=0.020013 (18.6s)
[Trial 24] lb=45, layers=3, d_model=128, heads=4, dff=256, drop=0.02182505499600511, opt=adam, lr=0.004500501499417736, bs=64  => val_loss=2.018157 (49.6s)
[Trial 25] lb=90, layers=1, d_model=64, heads=2, dff=256, drop=0.018280967077970955, opt=adam, lr=0.0003798989834296832, bs=128  => val_loss=0.077747 (57.1s)
[Trial 26] lb=45, layers=1, d_model=64, heads=4, dff=256, drop=0.21995683272048558, opt=adam, lr=0.001989780649746322, bs=128  => val_loss=0.017173 (17.8s)
[Trial 27] lb=45, layers=2, d_model=96, heads=8, dff=384, drop=0.17165930393761816, opt=adam, lr=0.0008516741937643491, bs=128  => val_loss=0.013539 (67.5s)
[Trial 28] lb=90, layers=1, d_model=64, heads=8, dff=128, drop=0.2488406101315022, opt=adam, lr=0.00014004996727905687, bs=64  => val_loss=0.015433 (109.9s)
[Trial 29] lb=90, layers=3, d_model=64, heads=8, dff=192, drop=0.21327365256767492, opt=adam, lr=0.0005848684623783666, bs=128  => val_loss=0.052100 (253.3s)
[Trial 30] lb=90, layers=3, d_model=128, heads=2, dff=384, drop=0.23032978910892826, opt=adam, lr=0.0011940747346721881, bs=64  => val_loss=0.005479 (689.8s)
[Trial 31] lb=60, layers=2, d_model=64, heads=2, dff=128, drop=0.11728824145386979, opt=adam, lr=0.0004149195465951722, bs=128  => val_loss=0.022743 (60.3s)
[Trial 32] lb=90, layers=2, d_model=128, heads=2, dff=384, drop=0.292575181539592, opt=adam, lr=0.0007150353649316262, bs=64  => val_loss=0.023832 (152.6s)
[Trial 33] lb=45, layers=3, d_model=128, heads=2, dff=256, drop=0.24545639636358382, opt=adam, lr=0.00041318080925395814, bs=64  => val_loss=0.004791 (356.6s)
[Trial 34] lb=60, layers=3, d_model=64, heads=8, dff=256, drop=0.13186319881184266, opt=adam, lr=0.00124077887575486, bs=128  => val_loss=0.021718 (130.1s)
[Trial 35] lb=60, layers=1, d_model=64, heads=4, dff=128, drop=0.1984096672918937, opt=adam, lr=0.0010086840722641388, bs=64  => val_loss=0.013289 (41.4s)
[Trial 36] lb=60, layers=1, d_model=64, heads=8, dff=256, drop=0.014468082722834107, opt=adam, lr=0.00022148747262117194, bs=32  => val_loss=0.025957 (150.4s)
[Trial 37] lb=30, layers=1, d_model=128, heads=4, dff=256, drop=0.05795527158350563, opt=adam, lr=0.00013997932588358776, bs=32  => val_loss=0.040438 (43.6s)
[Trial 38] lb=90, layers=2, d_model=128, heads=4, dff=512, drop=0.02635247699423492, opt=adam, lr=0.004809672637393132, bs=32  => val_loss=0.013903 (243.5s)
[Trial 39] lb=90, layers=1, d_model=128, heads=2, dff=384, drop=0.20505222010096688, opt=adam, lr=0.0009389812991849425, bs=32  => val_loss=0.035289 (69.4s)
[Trial 40] lb=30, layers=1, d_model=32, heads=2, dff=96, drop=0.22417768042534156, opt=adam, lr=0.0036600464943239886, bs=32  => val_loss=0.028031 (27.7s)
[Trial 41] lb=45, layers=1, d_model=64, heads=4, dff=128, drop=0.022516241875095544, opt=adam, lr=0.0001860038474194413, bs=128  => val_loss=0.042413 (47.9s)
[Trial 42] lb=45, layers=3, d_model=96, heads=2, dff=288, drop=0.10375955180702776, opt=adam, lr=0.00011576419964225703, bs=32  => val_loss=0.019365 (172.7s)
[Trial 43] lb=30, layers=3, d_model=128, heads=8, dff=512, drop=0.1466841563927242, opt=adam, lr=0.002797340743995813, bs=32  => val_loss=0.005873 (202.6s)
[Trial 44] lb=60, layers=2, d_model=128, heads=2, dff=384, drop=0.056415563390094986, opt=adam, lr=0.00020689912673517287, bs=64  => val_loss=0.013692 (224.4s)
[Trial 45] lb=90, layers=2, d_model=64, heads=8, dff=256, drop=0.10618238059017689, opt=adam, lr=0.00019822720847587663, bs=128  => val_loss=0.026925 (229.1s)
[Trial 46] lb=30, layers=1, d_model=64, heads=8, dff=256, drop=0.26468271227927787, opt=adam, lr=0.0037142954405059206, bs=128  => val_loss=0.010354 (21.7s)
[Trial 47] lb=60, layers=1, d_model=128, heads=4, dff=384, drop=0.15983980255213578, opt=adam, lr=0.0001210605352434013, bs=128  => val_loss=0.015785 (136.5s)
[Trial 48] lb=45, layers=1, d_model=32, heads=2, dff=96, drop=0.2137274736253229, opt=adam, lr=0.0002805350243582136, bs=64  => val_loss=0.034089 (12.4s)
[Trial 49] lb=60, layers=2, d_model=32, heads=4, dff=128, drop=0.14141475969092676, opt=adam, lr=0.00397833777725398, bs=128  => val_loss=0.011562 (60.1s)
[Trial 50] lb=30, layers=2, d_model=64, heads=4, dff=192, drop=0.1056900616485619, opt=adam, lr=0.00034721076715613336, bs=64  => val_loss=0.013379 (53.1s)

Best val_loss: 0.004044606816023588
Best params:
 {
  "lookback": 30,
  "d_model": 128,
  "num_heads": 2,
  "dff": 256,
  "num_layers": 3,
  "dropout": 0.15282611922146372,
  "optimizer": "adam",
  "lr": 0.0004030307529260446,
  "batch_size": 128,
  "epochs": 70,
  "patience": 9
}
--------------------------------------------------------------------------------
Cell 11
Cell type: code
-- Code --
# ---------------- Retrain best on TRAIN+VAL & Evaluate on TEST ----------------
lb = best["params"]["lookback"]
X_trainval_s = pd.concat([X_train_s, X_val_s], axis=0)
y_trainval_s = pd.concat([y_train_s, y_val_s], axis=0)

X_trv, y_trv, _ = make_windows(X_trainval_s, y_trainval_s, lb)
X_te,  y_te,  idx_te  = make_windows(X_test_s,      y_test_s,      lb)

best_model = build_transformer((lb, len(feature_cols)), best["params"])

callbacks_final = [
    EarlyStopping(monitor="val_loss", patience=PATIENCE, restore_best_weights=True),
    ModelCheckpoint(MODEL_BEST, monitor="val_loss", save_best_only=True),
    # ModelCheckpoint(WEIGHTS_BEST, monitor="val_loss", save_best_only=True, save_weights_only=True),
]

start_train = time.time()
hist_final = best_model.fit(
    X_trv, y_trv,
    validation_split=0.1,
    epochs=best["params"]["epochs"],
    batch_size=best["params"]["batch_size"],
    callbacks=callbacks_final,
    verbose=VERBOSE_TRAIN
)
end_train = time.time()
print(f"Final training time: {end_train - start_train:.2f} s")
-- Outputs --
[1] output_type: stream
Epoch 1/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m11s[0m 196ms/step - loss: 1.5820 - val_loss: 0.8121
Epoch 2/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 171ms/step - loss: 0.2968 - val_loss: 0.1015
Epoch 3/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 168ms/step - loss: 0.1918 - val_loss: 0.0512
Epoch 4/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.1546 - val_loss: 0.0356
Epoch 5/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.1318 - val_loss: 0.0337
Epoch 6/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 162ms/step - loss: 0.1075 - val_loss: 0.0424
Epoch 7/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0889 - val_loss: 0.0285
Epoch 8/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0762 - val_loss: 0.0409
Epoch 9/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 170ms/step - loss: 0.0730 - val_loss: 0.0230
Epoch 10/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 161ms/step - loss: 0.0669 - val_loss: 0.0246
Epoch 11/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 168ms/step - loss: 0.0539 - val_loss: 0.0229
Epoch 12/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0538 - val_loss: 0.0250
Epoch 13/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0475 - val_loss: 0.0191
Epoch 14/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0439 - val_loss: 0.0202
Epoch 15/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 161ms/step - loss: 0.0415 - val_loss: 0.0218
Epoch 16/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 161ms/step - loss: 0.0371 - val_loss: 0.0199
Epoch 17/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0329 - val_loss: 0.0145
Epoch 18/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 166ms/step - loss: 0.0337 - val_loss: 0.0172
Epoch 19/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0342 - val_loss: 0.0224
Epoch 20/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0321 - val_loss: 0.0137
Epoch 21/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0299 - val_loss: 0.0468
Epoch 22/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0292 - val_loss: 0.0212
Epoch 23/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 168ms/step - loss: 0.0293 - val_loss: 0.0128
Epoch 24/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 165ms/step - loss: 0.0275 - val_loss: 0.0129
Epoch 25/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 162ms/step - loss: 0.0263 - val_loss: 0.0168
Epoch 26/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 170ms/step - loss: 0.0260 - val_loss: 0.0094
Epoch 27/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0234 - val_loss: 0.0132
Epoch 28/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 170ms/step - loss: 0.0224 - val_loss: 0.0072
Epoch 29/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0249 - val_loss: 0.0098
Epoch 30/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0230 - val_loss: 0.0082
Epoch 31/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0255 - val_loss: 0.0187
Epoch 32/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 165ms/step - loss: 0.0230 - val_loss: 0.0126
Epoch 33/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0228 - val_loss: 0.0076
Epoch 34/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0203 - val_loss: 0.0083
Epoch 35/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0209 - val_loss: 0.0066
Epoch 36/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 162ms/step - loss: 0.0213 - val_loss: 0.0089
Epoch 37/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 167ms/step - loss: 0.0206 - val_loss: 0.0061
Epoch 38/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 170ms/step - loss: 0.0203 - val_loss: 0.0059
Epoch 39/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0201 - val_loss: 0.0081
Epoch 40/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0185 - val_loss: 0.0082
Epoch 41/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0190 - val_loss: 0.0125
Epoch 42/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0191 - val_loss: 0.0143
Epoch 43/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0185 - val_loss: 0.0064
Epoch 44/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0175 - val_loss: 0.0065
Epoch 45/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0166 - val_loss: 0.0073
Epoch 46/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 162ms/step - loss: 0.0165 - val_loss: 0.0089
Epoch 47/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0173 - val_loss: 0.0051
Epoch 48/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0180 - val_loss: 0.0057
Epoch 49/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 171ms/step - loss: 0.0192 - val_loss: 0.0050
Epoch 50/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 168ms/step - loss: 0.0164 - val_loss: 0.0045
Epoch 51/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 165ms/step - loss: 0.0172 - val_loss: 0.0154
Epoch 52/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 169ms/step - loss: 0.0162 - val_loss: 0.0039
Epoch 53/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0177 - val_loss: 0.0048
Epoch 54/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 163ms/step - loss: 0.0156 - val_loss: 0.0142
Epoch 55/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 167ms/step - loss: 0.0165 - val_loss: 0.0045
Epoch 56/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0160 - val_loss: 0.0062
Epoch 57/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 164ms/step - loss: 0.0151 - val_loss: 0.0055
Epoch 58/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 175ms/step - loss: 0.0153 - val_loss: 0.0082
Epoch 59/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 170ms/step - loss: 0.0144 - val_loss: 0.0045
Epoch 60/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 172ms/step - loss: 0.0140 - val_loss: 0.0081
Epoch 61/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 171ms/step - loss: 0.0145 - val_loss: 0.0043
Epoch 62/70
[1m21/21[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m4s[0m 172ms/step - loss: 0.0150 - val_loss: 0.0054
Final training time: 224.49 s
--------------------------------------------------------------------------------
Cell 12
Cell type: code
-- Code --
# Inference on test
start_test = time.time()
yhat_s = best_model.predict(X_te, verbose=1)
yhat   = y_scaler.inverse_transform(yhat_s).squeeze()
end_test = time.time()
print(f"Testing (inference) time: {end_test - start_test:.4f} s")
-- Outputs --
[1] output_type: stream
[1m22/22[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 32ms/step
Testing (inference) time: 1.0381 s
--------------------------------------------------------------------------------
Cell 13
Cell type: code
-- Code --
# Align & metrics
actual = df[TARGET_COL]
actual_test = actual.loc[idx_te]
pred_test   = pd.Series(yhat, index=idx_te, name="Pred")

mse  = mean_squared_error(actual_test.values, pred_test.values)
mae  = mean_absolute_error(actual_test.values, pred_test.values)
rmse = np.sqrt(mse)
mape = mean_absolute_percentage_error(actual_test.values, pred_test.values)
r2   = r2_score(actual_test.values, pred_test.values)

metrics_df = pd.DataFrame(
    [[mse, mae, rmse, mape, r2]],
    columns=["MSE", "MAE", "RMSE", "MAPE", "RÂ²"],
    index=["Test (best cfg)"]
)
pd.set_option("display.float_format", "{:.4f}".format)
print("\n=== Metrics (Final Best Config, Transformer) ===")
print(metrics_df)
-- Outputs --
[1] output_type: stream

=== Metrics (Final Best Config, Transformer) ===
                       MSE      MAE     RMSE   MAPE     RÂ²
Test (best cfg) 76440.5771 240.4978 276.4789 0.0334 0.3219
--------------------------------------------------------------------------------
Cell 14
Cell type: code
-- Code --
residuals = pd.Series(actual_test.values - pred_test.values, index=idx_te, name="Residuals")
--------------------------------------------------------------------------------
Cell 15
Cell type: code
-- Code --
plt.figure(figsize=(12, 5))
plt.plot(actual_test.index, actual_test.values, label="Actual (Test horizon)", linewidth=1.5)
plt.plot(pred_test.index,   pred_test.values,   label="Predicted (Test)", linewidth=1.5)
plt.title("Actual vs Predicted â€” Test (Best Random Search Config, Transformer)")
plt.xlabel("Date"); plt.ylabel(TARGET_COL)
plt.legend(); plt.tight_layout(); plt.show()

plt.figure(figsize=(12, 4))
plt.plot(residuals.index, residuals.values, linewidth=1)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals Over Time (Test) â€” Transformer (Best Config)")
plt.xlabel("Date"); plt.ylabel("Residual = Actual - Predicted")
plt.tight_layout(); plt.show()

plt.figure(figsize=(7, 5))
plt.hist(residuals.values, bins=50, edgecolor="black", alpha=0.8)
plt.title("Residuals Histogram (Test) â€” Transformer")
plt.xlabel("Residual"); plt.ylabel("Frequency")
plt.tight_layout(); plt.show()

plt.figure(figsize=(7, 5))
plt.scatter(pred_test.values, residuals.values, s=10, alpha=0.6)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals vs Fitted (Test) â€” Transformer")
plt.xlabel("Predicted (Fitted)"); plt.ylabel("Residual")
plt.tight_layout(); plt.show()

plt.figure(figsize=(8, 5))
plt.plot(hist_final.history["loss"], label="Train Loss")
plt.plot(hist_final.history["val_loss"], label="Val Loss")
plt.title("Training vs Validation Loss (Best Config, Transformer)")
plt.xlabel("Epoch"); plt.ylabel("MSE Loss")
plt.legend(); plt.tight_layout(); plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[2] output_type: display_data
<Figure size 1200x400 with 1 Axes>
[3] output_type: display_data
<Figure size 700x500 with 1 Axes>
[4] output_type: display_data
<Figure size 700x500 with 1 Axes>
[5] output_type: display_data
<Figure size 800x500 with 1 Axes>
--------------------------------------------------------------------------------
Cell 16
Cell type: code
-- Code --
# === CONFIGURATION ===
results_dir = os.path.join("..", "Results")
predicted_path = os.path.join(results_dir, "ALL_PREDICTED.csv")
metrics_path = os.path.join(results_dir, "ALL_METRICS.csv")

# Manual model name (since __file__ isn't available in notebooks)
model = "transformer_rs"
model_name = f"{model}_{RANDOM_SEED}"   # change this for each notebook (e.g., GRU_Baseline)
print("Model Name for Documentation:", model_name)

# Create Results directory if not exists
os.makedirs(results_dir, exist_ok=True)

# ==========================================
# 1ï¸âƒ£ PREPARE AND ALIGN TESTING DATAFRAME
# ==========================================

# Convert dates
test_dates = test_df.index.to_series().reset_index(drop=True)
actual_values = test_df[TARGET_COL].values

# If ALL_PREDICTED doesn't exist, create the base file
if not os.path.exists(predicted_path):
    print("Creating ALL_PREDICTED.csv ...")
    base_df = pd.DataFrame({
        "date": test_dates,
        "actual": actual_values
    })
    base_df.to_csv(predicted_path, index=False)

# Load and ensure datetime consistency
all_pred_df = pd.read_csv(predicted_path)
all_pred_df["date"] = pd.to_datetime(all_pred_df["date"])

# Ensure the file covers full test range (in case it was made from smaller data)
base_df = pd.DataFrame({
    "date": test_dates,
    "actual": actual_values
})
# Outer merge to make sure we have the full timeline
all_pred_df = pd.merge(base_df, all_pred_df, on=["date", "actual"], how="outer")

# Create new prediction column (aligned to date)
pred_series = pd.Series(pred_test.values, index=pd.to_datetime(idx_te), name=model_name)
pred_series = pred_series.reindex(all_pred_df["date"])  # align by date

# Add or update the model column
all_pred_df[model_name] = pred_series.values

# Sort and save
all_pred_df = all_pred_df.sort_values("date").reset_index(drop=True)
all_pred_df.to_csv(predicted_path, index=False)
print(f"âœ… Predictions saved to {predicted_path}")

# ==========================================
# 2ï¸âƒ£ RECORD METRICS SUMMARY
# ==========================================
metrics_columns = [
    "timestamp",
    "model_name", 
    "seed",
    "mse",
    "mae",
    "rmse", 
    "mape",
    "r2_score",
    "picp",
    "mpiw",
    "winkler_score",
    "training_time_s",
    "testing_time_s", 
    "hpo_trial_s",
    "hpo_time_s"
]

# Create ALL_METRICS if missing
if not os.path.exists(metrics_path):
    print("Creating ALL_METRICS.csv ...")
    pd.DataFrame(columns=metrics_columns).to_csv(metrics_path, index=False)

# Current timestamp
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Extract metrics
# mse, mae, rmse, mape, r2 = metrics_test
picp = mpiw = winkler = 0

# Build metrics row
metrics_row = {
    "timestamp": timestamp,
    "model_name": model_name,
    "seed": RANDOM_SEED,
    "mse": mse,
    "mae": mae,
    "rmse": rmse,
    "mape": mape,
    "r2_score": r2,
    "picp": picp,
    "mpiw": mpiw,
    "winkler_score": winkler,
    "training_time_s": round(end_train - start_train, 4),
    "testing_time_s": round(end_test - start_test, 4),
    "hpo_trial_s": N_TRIALS,
    "hpo_time_s": round(hpo_end - hpo_start, 4),
}

# Append metrics
all_metrics_df = pd.read_csv(metrics_path)
all_metrics_df = pd.concat([all_metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)
all_metrics_df.to_csv(metrics_path, index=False)
print(f"âœ… Metrics appended to {metrics_path}")

print("\nğŸ“„ Documentation of predictions and metrics completed successfully.")
-- Outputs --
[1] output_type: stream
Model Name for Documentation: transformer_rs_271828183
âœ… Predictions saved to ..\Results\ALL_PREDICTED.csv
âœ… Metrics appended to ..\Results\ALL_METRICS.csv

ğŸ“„ Documentation of predictions and metrics completed successfully.
--------------------------------------------------------------------------------
