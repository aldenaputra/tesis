Notebook: Transformer_Optuna.ipynb
Kernelspec: {'display_name': 'tesis_env', 'language': 'python', 'name': 'python3'}

--------------------------------------------------------------------------------
Cell 1
Cell type: code
-- Code --
!pip install -q optuna optuna-integration[tfkeras]
-- Outputs --
[1] output_type: stream

[notice] A new release of pip is available: 23.1.2 -> 25.3
[notice] To update, run: python.exe -m pip install --upgrade pip
--------------------------------------------------------------------------------
Cell 2
Cell type: code
-- Code --
import time
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import optuna
import random
import os

from datetime import datetime
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

import tensorflow as tf
from tensorflow.keras import Model, Input, optimizers
from tensorflow.keras.layers import Dense, Dropout, LayerNormalization, MultiHeadAttention, Add, Lambda
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from optuna.integration import TFKerasPruningCallback
--------------------------------------------------------------------------------
Cell 3
Cell type: code
-- Code --
file_name = ".SEED.txt"
with open(file_name, "r") as file:
    content = file.read().strip()  # Read and remove any extra whitespace/newlines
    number = int(content)  # Use float() to support decimal; use int() if it's always an integer

print("Seed:", number)
print("Type:", type(number))
-- Outputs --
[1] output_type: stream
Seed: 271828183
Type: <class 'int'>
--------------------------------------------------------------------------------
Cell 4
Cell type: code
-- Code --
CSV_PATH   = "ALL_MERGED.csv"   # <--- set your CSV path
DATE_COL   = "Date"
TARGET_COL = "JKSE"
INCLUDE_TARGET_AS_FEATURE = True  # set False to exclude JKSE from X

TEST_SIZE  = 0.20
VAL_SIZE   = 0.10

N_TRIALS   = 50
RANDOM_SEED = number

print(RANDOM_SEED)
random.seed(RANDOM_SEED)
np.random.seed(RANDOM_SEED)
tf.random.set_seed(RANDOM_SEED)
os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)
-- Outputs --
[1] output_type: stream
271828183
--------------------------------------------------------------------------------
Cell 5
Cell type: code
-- Code --
df = pd.read_csv(CSV_PATH)
required = [
    "Date","Nickel_Fut","Coal_Fut_Newcastle","Palm_Oil_Fut",
    "USD_IDR","CNY_IDR","EUR_IDR","BTC_USD",
    "FTSE100","HANGSENG","NIKKEI225","SNP500","DOW30","SSE_Composite","JKSE"
]
missing = [c for c in required if c not in df.columns]
if missing:
    raise ValueError(f"Missing columns: {missing}")

df[DATE_COL] = pd.to_datetime(df[DATE_COL])
df = df.sort_values(DATE_COL).set_index(DATE_COL)
df = df.ffill().bfill()
df
-- Outputs --
[1] output_type: execute_result
            Nickel_Fut  Coal_Fut_Newcastle  Palm_Oil_Fut  USD_IDR  CNY_IDR  \
Date                                                                         
2015-05-01    13750.00               62.20          2136  12962.5  2089.78   
2015-05-02    13750.00               62.20          2136  12962.5  2089.78   
2015-05-03    13750.00               62.20          2136  12962.5  2089.78   
2015-05-04    13750.00               62.20          2136  12980.0  2090.51   
2015-05-05    14245.00               62.50          2136  13047.5  2102.39   
...                ...                 ...           ...      ...      ...   
2025-04-27    15482.88               93.75          4057  16830.0  2309.37   
2025-04-28    15637.63               95.60          3943  16855.0  2310.59   
2025-04-29    15564.00               97.25          3911  16760.0  2305.52   
2025-04-30    15342.50               97.50          3911  16600.0  2282.94   
2025-05-01    15218.63               97.50          3881  16600.0  2282.94   

            EUR_IDR       BTC_USD      FTSE100     HANGSENG    NIKKEI225  \
Date                                                                       
2015-05-01  14519.9    232.078995  6986.000000  28123.82031  19531.63086   
2015-05-02  14519.9    234.929993  6986.000000  28123.82031  19531.63086   
2015-05-03  14519.9    240.358002  6986.000000  28123.82031  19531.63086   
2015-05-04  14467.5    239.018005  6986.000000  28123.82031  19531.63086   
2015-05-05  14594.9    236.121002  6927.600098  27755.53906  19531.63086   
...             ...           ...          ...          ...          ...   
2025-04-27  19127.3  93754.843750  8415.299805  21980.74023  35705.73828   
2025-04-28  19251.8  94978.750000  8417.299805  21971.96094  35839.98828   
2025-04-29  19082.1  94284.789060  8463.500000  22008.10938  35839.98828   
2025-04-30  18805.3  94207.312500  8494.900391  22119.41016  36045.37891   
2025-05-01  18745.6  96492.335940  8496.799805  22119.41016  36452.30078   

                 SNP500        DOW30  SSE_Composite         JKSE  
Date                                                              
2015-05-01  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-02  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-03  2108.290039  18024.06055    4480.463867  5140.937012  
2015-05-04  2114.489990  18070.40039    4480.463867  5140.937012  
2015-05-05  2089.459961  17928.19922    4298.706055  5160.107422  
...                 ...          ...            ...          ...  
2025-04-27  5525.209961  40113.50000    3295.060059  6678.915039  
2025-04-28  5528.750000  40227.58984    3288.415039  6722.965820  
2025-04-29  5560.830078  40527.62109    3286.655029  6749.075195  
2025-04-30  5569.060059  40669.35938    3279.031006  6766.794922  
2025-05-01  5604.140137  40752.96094    3279.031006  6766.794922  

[3654 rows x 14 columns]
--------------------------------------------------------------------------------
Cell 6
Cell type: code
-- Code --
plt.figure(figsize=(12, 7))
plt.plot(df.index, df)
plt.title(f"Dataset Plot")
plt.xlabel("Date"); plt.ylabel("Price")
plt.legend(labels=df.columns)
plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x700 with 1 Axes>
--------------------------------------------------------------------------------
Cell 7
Cell type: code
-- Code --
n = len(df)
test_n = int(np.floor(TEST_SIZE * n))
trainval_n = n - test_n
val_n = int(np.floor(VAL_SIZE * trainval_n))
train_n = trainval_n - val_n

train_df = df.iloc[:train_n].copy()
val_df   = df.iloc[train_n:train_n + val_n].copy()
test_df  = df.iloc[train_n + val_n:].copy()

print(f"Train: {len(train_df)} ({len(train_df)/len(df)*100:.1f}%) | "
      f"Validation: {len(val_df)} ({len(val_df)/len(df)*100:.1f}%) | "
      f"Test: {len(test_df)} ({len(test_df)/len(df)*100:.1f}%)")
-- Outputs --
[1] output_type: stream
Train: 2632 (72.0%) | Validation: 292 (8.0%) | Test: 730 (20.0%)
--------------------------------------------------------------------------------
Cell 8
Cell type: code
-- Code --
if INCLUDE_TARGET_AS_FEATURE:
    feature_cols = df.columns.tolist()          # includes TARGET_COL (JKSE)
else:
    feature_cols = [c for c in df.columns if c != TARGET_COL]
--------------------------------------------------------------------------------
Cell 9
Cell type: code
-- Code --
# feature_cols = [c for c in df.columns if c != TARGET_COL]
X_scaler = StandardScaler()
y_scaler = StandardScaler()
X_scaler.fit(train_df[feature_cols])
y_scaler.fit(train_df[[TARGET_COL]])

def scale_block(block):
    X = X_scaler.transform(block[feature_cols])
    y = y_scaler.transform(block[[TARGET_COL]])
    return pd.DataFrame(X, index=block.index, columns=feature_cols), pd.DataFrame(y, index=block.index, columns=[TARGET_COL])

X_train_s, y_train_s = scale_block(train_df)
X_val_s,   y_val_s   = scale_block(val_df)
X_test_s,  y_test_s  = scale_block(test_df)
--------------------------------------------------------------------------------
Cell 10
Cell type: code
-- Code --
def make_windows(X_df, y_df, lookback):
    X_vals = X_df.values
    y_vals = y_df.values.squeeze()
    idx = X_df.index

    X_list, y_list, idx_list = [], [], []
    for i in range(lookback, len(X_df)):
        X_list.append(X_vals[i - lookback:i, :])
        y_list.append(y_vals[i])
        idx_list.append(idx[i])

    X_arr = np.array(X_list, dtype=np.float32)
    y_arr = np.array(y_list, dtype=np.float32)
    idx_arr = np.array(idx_list)
    return X_arr, y_arr, idx_arr
--------------------------------------------------------------------------------
Cell 11
Cell type: code
-- Code --
def compute_metrics(y_true, y_pred):
    mse  = mean_squared_error(y_true, y_pred)
    mae  = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mse)
    mape = mean_absolute_percentage_error(y_true, y_pred)
    r2   = r2_score(y_true, y_pred)
    return [mse, mae, rmse, mape, r2]
--------------------------------------------------------------------------------
Cell 12
Cell type: code
-- Code --
class SinusoidalPositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, d_model, **kwargs):
        super().__init__(**kwargs)
        self.d_model = d_model

    def call(self, x):
        # x: (batch, L, d_model)
        L = tf.shape(x)[1]
        d = self.d_model + (self.d_model % 2)  # ensure even
        pos = tf.cast(tf.range(L)[:, None], tf.float32)                       # (L,1)
        i = tf.cast(tf.range(d)[None, :], tf.float32)                         # (1,d)
        angle_rates = 1.0 / tf.pow(10000.0, (2*(i//2))/d)
        angles = pos * angle_rates                                            # (L,d)
        sines = tf.sin(angles[:, 0::2])
        coses = tf.cos(angles[:, 1::2])
        pe = tf.concat([sines, coses], axis=-1)[:, :self.d_model]             # (L,d_model)
        return x + pe[None, :, :]
--------------------------------------------------------------------------------
Cell 13
Cell type: code
-- Code --
def encoder_block(x, num_heads, d_model, dff, dropout_rate):
    attn_out = MultiHeadAttention(num_heads=num_heads, key_dim=d_model//num_heads)(
        x, x, use_causal_mask=True
    )
    x = Add()([x, Dropout(dropout_rate)(attn_out)])
    x = LayerNormalization(epsilon=1e-6)(x)

    ff = Dense(dff, activation="relu")(x)
    ff = Dropout(dropout_rate)(ff)
    ff = Dense(d_model)(ff)

    x = Add()([x, Dropout(dropout_rate)(ff)])
    x = LayerNormalization(epsilon=1e-6)(x)
    return x

def build_transformer_model(lookback, n_features, d_model, num_heads, dff, num_layers, dropout, lr):
    inp = Input(shape=(lookback, n_features))
    x = Dense(d_model)(inp)
    x = SinusoidalPositionalEncoding(d_model)(x)
    for _ in range(num_layers):
        x = encoder_block(x, num_heads=num_heads, d_model=d_model, dff=dff, dropout_rate=dropout)
    x = Lambda(lambda t: t[:, -1, :])(x)  # last step
    out = Dense(1)(x)
    model = Model(inputs=inp, outputs=out)
    model.compile(optimizer=optimizers.Adam(learning_rate=lr), loss="mse")
    return model
--------------------------------------------------------------------------------
Cell 14
Cell type: code
-- Code --
def objective(trial):
    lookback = trial.suggest_categorical("lookback", [30, 45, 60, 90])
    d_model  = trial.suggest_categorical("d_model", [32, 64, 96, 128])
    valid_heads = [h for h in (2, 4, 8) if d_model % h == 0 and d_model // h >= 8]
    num_heads = trial.suggest_categorical(f"num_heads_d{d_model}", valid_heads)
    dff       = trial.suggest_int("dff", 2*d_model, 4*d_model, step=d_model)  # 2x..4x
    num_layers= trial.suggest_int("num_layers", 1, 3)
    dropout   = trial.suggest_float("dropout", 0.0, 0.3)
    lr        = trial.suggest_float("lr", 1e-4, 5e-3, log=True)
    batch_size= trial.suggest_categorical("batch_size", [32, 64, 128])
    epochs    = trial.suggest_int("epochs", 30, 90, step=10)
    patience  = trial.suggest_int("patience", 5, 10)

    X_tr, y_tr, _ = make_windows(X_train_s, y_train_s, lookback)
    X_va, y_va, _ = make_windows(X_val_s,   y_val_s,   lookback)

    model = build_transformer_model(
        lookback=lookback,
        n_features=len(feature_cols),
        d_model=d_model,
        num_heads=num_heads,
        dff=dff,
        num_layers=num_layers,
        dropout=dropout,
        lr=lr
    )

    callbacks = [
        EarlyStopping(monitor="val_loss", patience=patience, restore_best_weights=True),
        TFKerasPruningCallback(trial, monitor="val_loss"),
    ]

    history = model.fit(
        X_tr, y_tr,
        validation_data=(X_va, y_va),
        epochs=epochs,
        batch_size=batch_size,
        verbose=0,
        callbacks=callbacks
    )
    return min(history.history["val_loss"])
--------------------------------------------------------------------------------
Cell 15
Cell type: code
-- Code --
sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)
pruner  = optuna.pruners.MedianPruner(n_startup_trials=15, n_warmup_steps=10)
study = optuna.create_study(direction="minimize", sampler=sampler, pruner=pruner)

print("\nStarting Optuna study...")
start_opt = time.time()
study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=True)
end_opt = time.time()
print(f"Optuna finished in {end_opt - start_opt:.4f} seconds")
print("Best trial:", study.best_trial.number)
print("Best val_loss:", study.best_value)
print("Best params:", study.best_params)

best = study.best_params
BEST_LOOKBACK = best["lookback"]
-- Outputs --
[1] output_type: stream
[I 2025-10-28 10:47:13,669] A new study created in memory with name: no-name-c4b3262b-a29c-4754-9e48-618b11f663f1
[2] output_type: stream

Starting Optuna study...
[3] output_type: display_data
  0%|          | 0/50 [00:00<?, ?it/s]
[4] output_type: stream
WARNING:tensorflow:From C:\Users\Alden\AppData\Roaming\Python\Python311\site-packages\keras\src\backend\tensorflow\core.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.

[I 2025-10-28 10:47:41,811] Trial 0 finished with value: 0.5897206664085388 and parameters: {'lookback': 60, 'd_model': 32, 'num_heads_d32': 4, 'dff': 128, 'num_layers': 1, 'dropout': 0.16578413617917212, 'lr': 0.00011434542851382032, 'batch_size': 128, 'epochs': 30, 'patience': 6}. Best is trial 0 with value: 0.5897206664085388.
[I 2025-10-28 10:54:17,954] Trial 1 finished with value: 0.00734154786914587 and parameters: {'lookback': 90, 'd_model': 128, 'num_heads_d128': 4, 'dff': 384, 'num_layers': 3, 'dropout': 0.08379714834249066, 'lr': 0.0019588571406294005, 'batch_size': 64, 'epochs': 50, 'patience': 8}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:55:19,086] Trial 2 finished with value: 0.013506490737199783 and parameters: {'lookback': 30, 'd_model': 32, 'num_heads_d32': 2, 'dff': 96, 'num_layers': 3, 'dropout': 0.22933987059232444, 'lr': 0.0009378787469319193, 'batch_size': 64, 'epochs': 70, 'patience': 7}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:55:48,946] Trial 3 finished with value: 0.14790424704551697 and parameters: {'lookback': 30, 'd_model': 32, 'num_heads_d32': 4, 'dff': 96, 'num_layers': 2, 'dropout': 0.014468082722834107, 'lr': 0.00022148747262117196, 'batch_size': 32, 'epochs': 90, 'patience': 9}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:56:37,302] Trial 4 finished with value: 0.01898391731083393 and parameters: {'lookback': 60, 'd_model': 128, 'num_heads_d128': 2, 'dff': 384, 'num_layers': 1, 'dropout': 0.26468271227927787, 'lr': 0.0037142954405059254, 'batch_size': 128, 'epochs': 40, 'patience': 7}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:56:56,019] Trial 5 finished with value: 0.024754252284765244 and parameters: {'lookback': 30, 'd_model': 32, 'num_heads_d32': 4, 'dff': 64, 'num_layers': 1, 'dropout': 0.2539835854026779, 'lr': 0.0004215237645851479, 'batch_size': 128, 'epochs': 70, 'patience': 5}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:57:16,295] Trial 6 finished with value: 0.02573956549167633 and parameters: {'lookback': 90, 'd_model': 32, 'num_heads_d32': 4, 'dff': 96, 'num_layers': 1, 'dropout': 0.20045362848594395, 'lr': 0.004965330183724395, 'batch_size': 32, 'epochs': 40, 'patience': 7}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:58:36,921] Trial 7 finished with value: 0.12479782104492188 and parameters: {'lookback': 90, 'd_model': 96, 'num_heads_d96': 4, 'dff': 288, 'num_layers': 3, 'dropout': 0.28808209191118117, 'lr': 0.00013703093446825682, 'batch_size': 64, 'epochs': 80, 'patience': 5}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 10:59:10,417] Trial 8 finished with value: 0.018017923459410667 and parameters: {'lookback': 45, 'd_model': 32, 'num_heads_d32': 2, 'dff': 64, 'num_layers': 3, 'dropout': 0.2717889677899436, 'lr': 0.00135038511237591, 'batch_size': 64, 'epochs': 80, 'patience': 7}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:00:19,676] Trial 9 finished with value: 0.014244187623262405 and parameters: {'lookback': 60, 'd_model': 96, 'num_heads_d96': 4, 'dff': 192, 'num_layers': 2, 'dropout': 0.2283382960568189, 'lr': 0.002485893208305801, 'batch_size': 128, 'epochs': 60, 'patience': 6}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:06:07,176] Trial 10 finished with value: 0.009208704344928265 and parameters: {'lookback': 90, 'd_model': 128, 'num_heads_d128': 4, 'dff': 512, 'num_layers': 3, 'dropout': 0.047691471886074276, 'lr': 0.0016857931946024191, 'batch_size': 64, 'epochs': 50, 'patience': 10}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:09:11,253] Trial 11 finished with value: 0.0267314612865448 and parameters: {'lookback': 90, 'd_model': 128, 'num_heads_d128': 4, 'dff': 512, 'num_layers': 3, 'dropout': 0.06657438489859936, 'lr': 0.0018034113387255626, 'batch_size': 64, 'epochs': 50, 'patience': 10}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:16:05,225] Trial 12 finished with value: 0.009915615431964397 and parameters: {'lookback': 90, 'd_model': 128, 'num_heads_d128': 4, 'dff': 512, 'num_layers': 2, 'dropout': 0.10507017197774876, 'lr': 0.0006242059320235998, 'batch_size': 64, 'epochs': 50, 'patience': 9}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:20:25,166] Trial 13 finished with value: 0.022130683064460754 and parameters: {'lookback': 90, 'd_model': 64, 'num_heads_d64': 4, 'dff': 192, 'num_layers': 3, 'dropout': 0.07041285418186569, 'lr': 0.0020422916064223484, 'batch_size': 64, 'epochs': 50, 'patience': 10}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:22:33,217] Trial 14 finished with value: 0.009126429446041584 and parameters: {'lookback': 45, 'd_model': 128, 'num_heads_d128': 8, 'dff': 384, 'num_layers': 3, 'dropout': 0.005111565067404587, 'lr': 0.0009894435219639814, 'batch_size': 64, 'epochs': 30, 'patience': 9}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:24:34,646] Trial 15 finished with value: 0.010731385089457035 and parameters: {'lookback': 45, 'd_model': 128, 'num_heads_d128': 8, 'dff': 384, 'num_layers': 2, 'dropout': 0.006032494864491161, 'lr': 0.0008488108615975482, 'batch_size': 64, 'epochs': 30, 'patience': 8}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:25:22,844] Trial 16 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:27:43,642] Trial 17 finished with value: 0.01150432787835598 and parameters: {'lookback': 45, 'd_model': 128, 'num_heads_d128': 8, 'dff': 384, 'num_layers': 2, 'dropout': 0.1265089716841502, 'lr': 0.001094318856544591, 'batch_size': 64, 'epochs': 30, 'patience': 8}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:29:09,760] Trial 18 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:32:17,876] Trial 19 finished with value: 0.011257929727435112 and parameters: {'lookback': 45, 'd_model': 128, 'num_heads_d128': 2, 'dff': 384, 'num_layers': 3, 'dropout': 0.1548714003257855, 'lr': 0.002645534806258255, 'batch_size': 32, 'epochs': 40, 'patience': 9}. Best is trial 1 with value: 0.00734154786914587.
[I 2025-10-28 11:35:37,058] Trial 20 finished with value: 0.007028078660368919 and parameters: {'lookback': 90, 'd_model': 64, 'num_heads_d64': 8, 'dff': 256, 'num_layers': 2, 'dropout': 0.08701668685468227, 'lr': 0.003183944865842332, 'batch_size': 64, 'epochs': 30, 'patience': 8}. Best is trial 20 with value: 0.007028078660368919.
[I 2025-10-28 11:38:43,296] Trial 21 finished with value: 0.007964270189404488 and parameters: {'lookback': 90, 'd_model': 64, 'num_heads_d64': 8, 'dff': 192, 'num_layers': 2, 'dropout': 0.09059092892552062, 'lr': 0.00362398217725273, 'batch_size': 64, 'epochs': 30, 'patience': 8}. Best is trial 20 with value: 0.007028078660368919.
[I 2025-10-28 11:40:02,710] Trial 22 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:42:16,464] Trial 23 finished with value: 0.015442262403666973 and parameters: {'lookback': 90, 'd_model': 64, 'num_heads_d64': 8, 'dff': 192, 'num_layers': 2, 'dropout': 0.10202294861731911, 'lr': 0.004803182195143478, 'batch_size': 64, 'epochs': 30, 'patience': 8}. Best is trial 20 with value: 0.007028078660368919.
[I 2025-10-28 11:43:31,802] Trial 24 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:44:40,651] Trial 25 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:46:02,093] Trial 26 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:47:06,772] Trial 27 pruned. Trial was pruned at epoch 13.
[I 2025-10-28 11:47:40,059] Trial 28 finished with value: 0.029215345159173012 and parameters: {'lookback': 90, 'd_model': 96, 'num_heads_d96': 2, 'dff': 288, 'num_layers': 1, 'dropout': 0.1280164436343486, 'lr': 0.004154265603963736, 'batch_size': 128, 'epochs': 60, 'patience': 8}. Best is trial 20 with value: 0.007028078660368919.
[I 2025-10-28 11:48:03,968] Trial 29 finished with value: 0.0064508430659770966 and parameters: {'lookback': 60, 'd_model': 64, 'num_heads_d64': 2, 'dff': 192, 'num_layers': 1, 'dropout': 0.18035211865969858, 'lr': 0.0028345996188188496, 'batch_size': 32, 'epochs': 30, 'patience': 6}. Best is trial 29 with value: 0.0064508430659770966.
[I 2025-10-28 11:48:21,203] Trial 30 finished with value: 0.021538639441132545 and parameters: {'lookback': 60, 'd_model': 64, 'num_heads_d64': 2, 'dff': 192, 'num_layers': 1, 'dropout': 0.19528691775150647, 'lr': 0.0014610920110356708, 'batch_size': 32, 'epochs': 40, 'patience': 6}. Best is trial 29 with value: 0.0064508430659770966.
[I 2025-10-28 11:49:03,542] Trial 31 finished with value: 0.004293706268072128 and parameters: {'lookback': 60, 'd_model': 64, 'num_heads_d64': 2, 'dff': 192, 'num_layers': 1, 'dropout': 0.18374733518286446, 'lr': 0.0028229907368294204, 'batch_size': 32, 'epochs': 30, 'patience': 7}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:49:23,252] Trial 32 finished with value: 0.01281686034053564 and parameters: {'lookback': 60, 'd_model': 64, 'num_heads_d64': 2, 'dff': 192, 'num_layers': 1, 'dropout': 0.17844711562585863, 'lr': 0.0028617943152787176, 'batch_size': 32, 'epochs': 30, 'patience': 6}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:49:43,026] Trial 33 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:49:58,058] Trial 34 finished with value: 0.0171501524746418 and parameters: {'lookback': 60, 'd_model': 64, 'num_heads_d64': 2, 'dff': 128, 'num_layers': 1, 'dropout': 0.14760309318099904, 'lr': 0.0030692533077945556, 'batch_size': 32, 'epochs': 40, 'patience': 5}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:50:25,917] Trial 35 finished with value: 0.005403799936175346 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.18879785854068634, 'lr': 0.0023148364867190325, 'batch_size': 32, 'epochs': 30, 'patience': 7}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:50:57,481] Trial 36 finished with value: 0.013445819728076458 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.18220955032575964, 'lr': 0.0025026954528776144, 'batch_size': 32, 'epochs': 30, 'patience': 7}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:52:04,077] Trial 37 finished with value: 0.005858008284121752 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.22601079539949398, 'lr': 0.004236538605747085, 'batch_size': 32, 'epochs': 30, 'patience': 6}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:52:46,744] Trial 38 finished with value: 0.0058359489776194096 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.23882556183991402, 'lr': 0.004584895863520672, 'batch_size': 32, 'epochs': 40, 'patience': 6}. Best is trial 31 with value: 0.004293706268072128.
[I 2025-10-28 11:53:13,167] Trial 39 finished with value: 0.004094381351023912 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.23924937691719658, 'lr': 0.004414680413871214, 'batch_size': 32, 'epochs': 40, 'patience': 5}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:53:57,826] Trial 40 finished with value: 0.007052829023450613 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.2450703810762505, 'lr': 0.004871939689564072, 'batch_size': 32, 'epochs': 40, 'patience': 5}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:54:23,826] Trial 41 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:54:59,806] Trial 42 finished with value: 0.006383433938026428 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.21589686000500213, 'lr': 0.0042466572264375805, 'batch_size': 32, 'epochs': 40, 'patience': 5}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:55:43,287] Trial 43 finished with value: 0.007527289446443319 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.2990432368830192, 'lr': 0.0041870330371933245, 'batch_size': 32, 'epochs': 30, 'patience': 6}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:56:22,325] Trial 44 finished with value: 0.010091925039887428 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.26638531606862387, 'lr': 0.00021116684201065355, 'batch_size': 32, 'epochs': 40, 'patience': 5}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:56:57,924] Trial 45 finished with value: 0.008136915974318981 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.25137720692231813, 'lr': 0.0049935848717004355, 'batch_size': 32, 'epochs': 50, 'patience': 7}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:57:30,952] Trial 46 finished with value: 0.005642300937324762 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.19830634541279782, 'lr': 0.002232420038280013, 'batch_size': 32, 'epochs': 30, 'patience': 6}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:57:49,867] Trial 47 pruned. Trial was pruned at epoch 10.
[I 2025-10-28 11:58:02,565] Trial 48 finished with value: 0.011385846883058548 and parameters: {'lookback': 30, 'd_model': 32, 'num_heads_d32': 2, 'dff': 64, 'num_layers': 1, 'dropout': 0.1920282037952479, 'lr': 0.00184866134063127, 'batch_size': 32, 'epochs': 80, 'patience': 5}. Best is trial 39 with value: 0.004094381351023912.
[I 2025-10-28 11:58:39,937] Trial 49 finished with value: 0.010434722527861595 and parameters: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.15969927923493582, 'lr': 0.00158498144147459, 'batch_size': 32, 'epochs': 90, 'patience': 6}. Best is trial 39 with value: 0.004094381351023912.
Optuna finished in 4286.2713 seconds
Best trial: 39
Best val_loss: 0.004094381351023912
Best params: {'lookback': 30, 'd_model': 96, 'num_heads_d96': 8, 'dff': 288, 'num_layers': 1, 'dropout': 0.23924937691719658, 'lr': 0.004414680413871214, 'batch_size': 32, 'epochs': 40, 'patience': 5}
--------------------------------------------------------------------------------
Cell 16
Cell type: code
-- Code --
X_train_w, y_train_w, idx_train = make_windows(X_train_s, y_train_s, BEST_LOOKBACK)
X_val_w,   y_val_w,   idx_val   = make_windows(X_val_s,   y_val_s,   BEST_LOOKBACK)
X_test_w,  y_test_w,  idx_test  = make_windows(X_test_s,  y_test_s,  BEST_LOOKBACK)
--------------------------------------------------------------------------------
Cell 17
Cell type: code
-- Code --
def safe_num_heads(best_params, d_model):
    # try to find any num_heads* key first
    for k, v in best_params.items():
        if k.startswith("num_heads"):
            return v
    # else pick the largest valid head count from {8,4,2}
    for h in (8, 4, 2):
        if d_model % h == 0 and d_model // h >= 8:
            return h
    raise ValueError(f"No valid num_heads for d_model={d_model}")

num_heads = safe_num_heads(best, best["d_model"])
--------------------------------------------------------------------------------
Cell 18
Cell type: code
-- Code --
final_model = build_transformer_model(
    lookback=BEST_LOOKBACK,
    n_features=len(feature_cols),
    d_model=best["d_model"],
    num_heads=num_heads,              # <-- use extracted value
    dff=best["dff"],
    num_layers=best["num_layers"],
    dropout=best["dropout"],
    lr=best["lr"]
)

callbacks = [
    EarlyStopping(monitor="val_loss", patience=best["patience"], restore_best_weights=True),
    ModelCheckpoint("Model Checkpoints/transformer_optuna_best.keras", monitor="val_loss", save_best_only=True)
]

print("\nRetraining final Transformer on TRAIN (validate on VAL)...")
start_train = time.time()
history = final_model.fit(
    X_train_w, y_train_w,
    validation_data=(X_val_w, y_val_w),
    epochs=best["epochs"],
    batch_size=best["batch_size"],
    verbose=1,
    callbacks=callbacks
)
end_train = time.time()
print(f"Final training time: {end_train - start_train:.4f} seconds")
-- Outputs --
[1] output_type: stream

Retraining final Transformer on TRAIN (validate on VAL)...
Epoch 1/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m6s[0m 25ms/step - loss: 2.2957 - val_loss: 0.1114
Epoch 2/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 21ms/step - loss: 0.0934 - val_loss: 0.0397
Epoch 3/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 21ms/step - loss: 0.0540 - val_loss: 0.0133
Epoch 4/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 20ms/step - loss: 0.0411 - val_loss: 0.0508
Epoch 5/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 20ms/step - loss: 0.0320 - val_loss: 0.0532
Epoch 6/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 21ms/step - loss: 0.0284 - val_loss: 0.0515
Epoch 7/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 20ms/step - loss: 0.0248 - val_loss: 0.0416
Epoch 8/40
[1m82/82[0m [32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ[0m[37m[0m [1m2s[0m 20ms/step - loss: 0.0237 - val_loss: 0.0168
Final training time: 17.8729 seconds
--------------------------------------------------------------------------------
Cell 19
Cell type: code
-- Code --
def predict_series(model, X_block, idx_block):
    yhat_s = model.predict(X_block, verbose=0)
    yhat = y_scaler.inverse_transform(yhat_s).squeeze()
    return pd.Series(yhat, index=idx_block, name="Pred")

pred_train = predict_series(final_model, X_train_w, idx_train)
pred_val   = predict_series(final_model, X_val_w,   idx_val)

start_test = time.time()
pred_test  = predict_series(final_model, X_test_w,  idx_test)
end_test = time.time()
print(f"Testing (inference) time: {end_test - start_test:.4f} seconds")
-- Outputs --
[1] output_type: stream
Testing (inference) time: 0.2565 seconds
--------------------------------------------------------------------------------
Cell 20
Cell type: code
-- Code --
actual       = df[TARGET_COL]
actual_train = actual.loc[idx_train]
actual_val   = actual.loc[idx_val]
actual_test  = actual.loc[idx_test]
--------------------------------------------------------------------------------
Cell 21
Cell type: code
-- Code --
def to_metrics_df(a_train, p_train, a_val, p_val, a_test, p_test):
    rows = [
        compute_metrics(a_train, p_train),
        compute_metrics(a_val,   p_val),
        compute_metrics(a_test,  p_test),
    ]
    return pd.DataFrame(rows, columns=["MSE","MAE","RMSE","MAPE","R¬≤"], index=["Train","Validation","Test"])

metrics_df = to_metrics_df(
    actual_train.values, pred_train.values,
    actual_val.values,   pred_val.values,
    actual_test.values,  pred_test.values
)

pd.set_option("display.float_format", "{:.4f}".format)
print("\n=== Metrics Summary (Transformer ‚Äî Optuna best) ===")
print(metrics_df.round(4))

metrics_test = compute_metrics(actual_test.values, pred_test.values)
-- Outputs --
[1] output_type: stream

=== Metrics Summary (Transformer ‚Äî Optuna best) ===
                  MSE      MAE     RMSE   MAPE     R¬≤
Train      19995.6268 115.6191 141.4059 0.0216 0.9602
Validation  6630.2935  63.9171  81.4266 0.0093 0.7554
Test       60019.2819 195.7717 244.9883 0.0270 0.4675
--------------------------------------------------------------------------------
Cell 22
Cell type: code
-- Code --
residuals = pd.Series(actual_test.values - pred_test.values, index=actual_test.index, name="Residuals")
--------------------------------------------------------------------------------
Cell 23
Cell type: code
-- Code --
# 1) All actual vs predicted
plt.figure(figsize=(12, 5))
plt.plot(actual.index, actual.values, label="Actual (JKSE)", linewidth=1)
plt.plot(pred_train.index, pred_train.values, label="Predicted (Train)", linewidth=1)
plt.plot(pred_val.index,   pred_val.values,   label="Predicted (Val)", linewidth=1)
plt.plot(pred_test.index,  pred_test.values,  label="Predicted (Test)", linewidth=1.5)
plt.title("All Actual vs. Predicted (Train/Val/Test) ‚Äî Transformer (Optuna best)")
plt.xlabel("Date"); plt.ylabel("JKSE")
plt.legend(); plt.tight_layout(); plt.show()

# 2) Actual vs predicted ‚Äî test horizon
plt.figure(figsize=(12, 5))
plt.plot(actual_test.index, actual_test.values, label="Actual (Test horizon)", linewidth=1.5)
plt.plot(pred_test.index,   pred_test.values,   label="Predicted (Test)", linewidth=1.5)
plt.title("Actual vs. Predicted ‚Äî Test Horizon (Transformer Optuna)")
plt.xlabel("Date"); plt.ylabel("JKSE")
plt.legend(); plt.tight_layout(); plt.show()

# 3) Residuals over time
plt.figure(figsize=(12, 4))
plt.plot(residuals.index, residuals.values, linewidth=1)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals Over Time (Test) ‚Äî Transformer (Optuna)")
plt.xlabel("Date"); plt.ylabel("Residual = Actual - Predicted")
plt.tight_layout(); plt.show()

# 4) Residual histogram
plt.figure(figsize=(7, 5))
plt.hist(residuals.values, bins=50, edgecolor="black", alpha=0.8)
plt.title("Residuals Histogram (Test) ‚Äî Transformer (Optuna)")
plt.xlabel("Residual"); plt.ylabel("Frequency")
plt.tight_layout(); plt.show()

# 5) Residuals vs fitted (test)
plt.figure(figsize=(7, 5))
plt.scatter(pred_test.values, residuals.values, s=10, alpha=0.6)
plt.axhline(0, linestyle="--", linewidth=1)
plt.title("Residuals vs. Fitted (Test) ‚Äî Transformer (Optuna)")
plt.xlabel("Predicted (Fitted)"); plt.ylabel("Residual")
plt.tight_layout(); plt.show()

# 6) Train vs Validation loss
plt.figure(figsize=(8, 5))
plt.plot(history.history["loss"], label="Train Loss")
plt.plot(history.history["val_loss"], label="Val Loss")
plt.title("Training vs Validation Loss ‚Äî Transformer (Optuna final)")
plt.xlabel("Epoch"); plt.ylabel("MSE Loss")
plt.legend(); plt.tight_layout(); plt.show()
-- Outputs --
[1] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[2] output_type: display_data
<Figure size 1200x500 with 1 Axes>
[3] output_type: display_data
<Figure size 1200x400 with 1 Axes>
[4] output_type: display_data
<Figure size 700x500 with 1 Axes>
[5] output_type: display_data
<Figure size 700x500 with 1 Axes>
[6] output_type: display_data
<Figure size 800x500 with 1 Axes>
--------------------------------------------------------------------------------
Cell 24
Cell type: code
-- Code --
# === CONFIGURATION ===
results_dir = os.path.join("..", "Results")
predicted_path = os.path.join(results_dir, "ALL_PREDICTED.csv")
metrics_path = os.path.join(results_dir, "ALL_METRICS.csv")

# Manual model name (since __file__ isn't available in notebooks)
model = "transformer_optuna"
model_name = f"{model}_{RANDOM_SEED}"   # change this for each notebook (e.g., GRU_Baseline)
print("Model Name for Documentation:", model_name)

# Create Results directory if not exists
os.makedirs(results_dir, exist_ok=True)

# ==========================================
# 1Ô∏è‚É£ PREPARE AND ALIGN TESTING DATAFRAME
# ==========================================

# Convert dates
test_dates = test_df.index.to_series().reset_index(drop=True)
actual_values = test_df[TARGET_COL].values

# If ALL_PREDICTED doesn't exist, create the base file
if not os.path.exists(predicted_path):
    print("Creating ALL_PREDICTED.csv ...")
    base_df = pd.DataFrame({
        "date": test_dates,
        "actual": actual_values
    })
    base_df.to_csv(predicted_path, index=False)

# Load and ensure datetime consistency
all_pred_df = pd.read_csv(predicted_path)
all_pred_df["date"] = pd.to_datetime(all_pred_df["date"])

# Ensure the file covers full test range (in case it was made from smaller data)
base_df = pd.DataFrame({
    "date": test_dates,
    "actual": actual_values
})
# Outer merge to make sure we have the full timeline
all_pred_df = pd.merge(base_df, all_pred_df, on=["date", "actual"], how="outer")

# Create new prediction column (aligned to date)
pred_series = pd.Series(pred_test.values, index=pd.to_datetime(idx_test), name=model_name)
pred_series = pred_series.reindex(all_pred_df["date"])  # align by date

# Add or update the model column
all_pred_df[model_name] = pred_series.values

# Sort and save
all_pred_df = all_pred_df.sort_values("date").reset_index(drop=True)
all_pred_df.to_csv(predicted_path, index=False)
print(f"‚úÖ Predictions saved to {predicted_path}")

# ==========================================
# 2Ô∏è‚É£ RECORD METRICS SUMMARY
# ==========================================
metrics_columns = [
    "timestamp",
    "model_name", 
    "seed",
    "mse",
    "mae",
    "rmse", 
    "mape",
    "r2_score",
    "picp",
    "mpiw",
    "winkler_score",
    "training_time_s",
    "testing_time_s", 
    "hpo_trial_s",
    "hpo_time_s"
]

# Create ALL_METRICS if missing
if not os.path.exists(metrics_path):
    print("Creating ALL_METRICS.csv ...")
    pd.DataFrame(columns=metrics_columns).to_csv(metrics_path, index=False)

# Current timestamp
timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

# Extract metrics
mse, mae, rmse, mape, r2 = metrics_test
picp = mpiw = winkler = 0

# Build metrics row
metrics_row = {
    "timestamp": timestamp,
    "model_name": model_name,
    "seed": RANDOM_SEED,
    "mse": mse,
    "mae": mae,
    "rmse": rmse,
    "mape": mape,
    "r2_score": r2,
    "picp": picp,
    "mpiw": mpiw,
    "winkler_score": winkler,
    "training_time_s": round(end_train - start_train, 4),
    "testing_time_s": round(end_test - start_test, 4),
    "hpo_trial_s": N_TRIALS,
    "hpo_time_s": round(end_opt - start_opt, 4),
}

# Append metrics
all_metrics_df = pd.read_csv(metrics_path)
all_metrics_df = pd.concat([all_metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)
all_metrics_df.to_csv(metrics_path, index=False)
print(f"‚úÖ Metrics appended to {metrics_path}")

print("\nüìÑ Documentation of predictions and metrics completed successfully.")
-- Outputs --
[1] output_type: stream
Model Name for Documentation: transformer_optuna_271828183
‚úÖ Predictions saved to ..\Results\ALL_PREDICTED.csv
‚úÖ Metrics appended to ..\Results\ALL_METRICS.csv

üìÑ Documentation of predictions and metrics completed successfully.
--------------------------------------------------------------------------------
