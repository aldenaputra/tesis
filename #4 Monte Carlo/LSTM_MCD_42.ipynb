{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a25925f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import random\n",
    "from typing import Dict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential, Input, optimizers\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05567278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- User Config ---------------------------\n",
    "CSV_PATH   = \"ALL_MERGED.csv\"\n",
    "DATE_COL   = \"Date\"\n",
    "TARGET_COL = \"JKSE\"\n",
    "INCLUDE_TARGET_AS_FEATURE = True\n",
    "\n",
    "TEST_SIZE  = 0.20\n",
    "VAL_SIZE   = 0.10\n",
    "\n",
    "N_TRIALS   = 50\n",
    "RANDOM_SEED = 42\n",
    "USE_PROGRESS_BAR = True\n",
    "\n",
    "# MC Dropout\n",
    "N_MC       = 100\n",
    "ALPHA      = 0.05\n",
    "USE_QUANTILES = True\n",
    "\n",
    "# Visuals\n",
    "DO_PLOTS   = True\n",
    "\n",
    "# Output\n",
    "CKPT_DIR   = \"Model Checkpoints\"\n",
    "CKPT_PATH  = os.path.join(CKPT_DIR, \"lstm_optuna_best.keras\")\n",
    "RESULTS_DIR = \"Results\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(RESULTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a428e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_global_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d3ab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Data Loading ---------------------------\n",
    "def load_dataset(csv_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(csv_path)\n",
    "    required = [\n",
    "        \"Date\", \"Nickel_Fut\", \"Coal_Fut_Newcastle\", \"Palm_Oil_Fut\",\n",
    "        \"USD_IDR\", \"CNY_IDR\", \"EUR_IDR\", \"BTC_USD\",\n",
    "        \"FTSE100\", \"HANGSENG\", \"NIKKEI225\", \"SNP500\", \"DOW30\", \"SSE_Composite\", TARGET_COL\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "    df[DATE_COL] = pd.to_datetime(df[DATE_COL])\n",
    "    df = df.sort_values(DATE_COL).set_index(DATE_COL)\n",
    "    df = df.ffill().bfill()\n",
    "    return df\n",
    "\n",
    "def split_df(df: pd.DataFrame, test_size: float, val_size: float):\n",
    "    n = len(df)\n",
    "    test_n = int(np.floor(test_size * n))\n",
    "    trainval_n = n - test_n\n",
    "    val_n = int(np.floor(val_size * trainval_n))\n",
    "    train_n = trainval_n - val_n\n",
    "\n",
    "    train_df = df.iloc[:train_n].copy()\n",
    "    val_df   = df.iloc[train_n:train_n + val_n].copy()\n",
    "    test_df  = df.iloc[train_n + val_n:].copy()\n",
    "    return train_df, val_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "557e0464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Scaling & Windows ---------------------------\n",
    "def get_feature_cols(df: pd.DataFrame, include_target: bool) -> list:\n",
    "    return df.columns.tolist() if include_target else [c for c in df.columns if c != TARGET_COL]\n",
    "\n",
    "def fit_scalers(train_df: pd.DataFrame, feature_cols: list):\n",
    "    X_scaler = StandardScaler().fit(train_df[feature_cols])\n",
    "    y_scaler = StandardScaler().fit(train_df[[TARGET_COL]])\n",
    "    return X_scaler, y_scaler\n",
    "\n",
    "def scale_block(block: pd.DataFrame, feature_cols: list, X_scaler: StandardScaler, y_scaler: StandardScaler):\n",
    "    X = X_scaler.transform(block[feature_cols])\n",
    "    y = y_scaler.transform(block[[TARGET_COL]])\n",
    "    return (pd.DataFrame(X, index=block.index, columns=feature_cols),\n",
    "            pd.DataFrame(y, index=block.index, columns=[TARGET_COL]))\n",
    "\n",
    "def make_windows(X_df: pd.DataFrame, y_df: pd.DataFrame, lookback: int):\n",
    "    X_vals = X_df.values\n",
    "    y_vals = y_df.values.squeeze()\n",
    "    idx = X_df.index\n",
    "\n",
    "    X_list, y_list, idx_list = [], [], []\n",
    "    for i in range(lookback, len(X_df)):\n",
    "        X_list.append(X_vals[i - lookback:i, :])\n",
    "        y_list.append(y_vals[i])\n",
    "        idx_list.append(idx[i])\n",
    "\n",
    "    X_arr = np.array(X_list, dtype=np.float32)\n",
    "    y_arr = np.array(y_list, dtype=np.float32)\n",
    "    idx_arr = np.array(idx_list)\n",
    "    return X_arr, y_arr, idx_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbb236fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Metrics ---------------------------\n",
    "def base_metrics(y_true, y_pred) -> Dict[str, float]:\n",
    "    mse  = mean_squared_error(y_true, y_pred)\n",
    "    mae  = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = float(np.sqrt(mse))\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    r2   = r2_score(y_true, y_pred)\n",
    "    return dict(MSE=float(mse), MAE=float(mae), RMSE=rmse, MAPE=float(mape), R2=float(r2))\n",
    "\n",
    "def uq_metrics(y_true, L, U, alpha=0.10) -> Dict[str, float]:\n",
    "    y = np.asarray(y_true); L = np.asarray(L); U = np.asarray(U)\n",
    "    cover = (y >= L) & (y <= U)\n",
    "    picp = float(cover.mean())\n",
    "    mpiw = float(np.mean(U - L))\n",
    "    penalty = np.where(y < L, (2/alpha)*(L - y),\n",
    "              np.where(y > U, (2/alpha)*(y - U), 0.0))\n",
    "    winkler = float(np.mean((U - L) + penalty))\n",
    "    return dict(PICP=picp, MPIW=mpiw, Winkler=winkler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e895fd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Model Builders ---------------------------\n",
    "def build_lstm_from_trial(trial, lookback, n_features):\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 2)\n",
    "    units1     = trial.suggest_int(\"units1\", 32, 256, step=32)\n",
    "    units2     = trial.suggest_int(\"units2\", 32, 256, step=32) if num_layers == 2 else None\n",
    "    dropout    = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
    "    lr         = trial.suggest_float(\"lr\", 1e-4, 5e-3, log=True)\n",
    "\n",
    "    m = Sequential()\n",
    "    m.add(Input(shape=(lookback, n_features)))\n",
    "    if num_layers == 2:\n",
    "        m.add(LSTM(units1, return_sequences=True))\n",
    "        m.add(Dropout(dropout))\n",
    "        m.add(LSTM(units2))\n",
    "    else:\n",
    "        m.add(LSTM(units1))\n",
    "    m.add(Dropout(dropout))  # same dropout used for MC\n",
    "    m.add(Dense(1))\n",
    "    m.compile(optimizer=optimizers.Adam(learning_rate=lr), loss=\"mse\")\n",
    "    return m\n",
    "\n",
    "def build_lstm_fixed(best_params: dict, lookback: int, n_features: int):\n",
    "    m = Sequential()\n",
    "    m.add(Input(shape=(lookback, n_features)))\n",
    "    if best_params.get(\"num_layers\", 1) == 2:\n",
    "        m.add(LSTM(best_params[\"units1\"], return_sequences=True))\n",
    "        m.add(Dropout(best_params[\"dropout\"]))\n",
    "        m.add(LSTM(best_params[\"units2\"]))\n",
    "    else:\n",
    "        m.add(LSTM(best_params[\"units1\"]))\n",
    "    m.add(Dropout(best_params[\"dropout\"]))\n",
    "    m.add(Dense(1))\n",
    "    m.compile(optimizer=optimizers.Adam(learning_rate=best_params[\"lr\"]), loss=\"mse\")\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb764bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Optuna Objective ---------------------------\n",
    "def make_objective(X_train_s, y_train_s, X_val_s, y_val_s, n_features):\n",
    "    def objective(trial):\n",
    "        lookback = trial.suggest_categorical(\"lookback\", [30, 45, 60, 90])\n",
    "        X_tr, y_tr, _ = make_windows(X_train_s, y_train_s, lookback)\n",
    "        X_va, y_va, _ = make_windows(X_val_s,   y_val_s,   lookback)\n",
    "\n",
    "        batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128])\n",
    "        epochs     = trial.suggest_int(\"epochs\", 30, 90, step=10)\n",
    "        patience   = trial.suggest_int(\"patience\", 5, 10)\n",
    "\n",
    "        model = build_lstm_from_trial(trial, lookback, n_features=n_features)\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=patience, restore_best_weights=True),\n",
    "            TFKerasPruningCallback(trial, monitor=\"val_loss\"),\n",
    "        ]\n",
    "\n",
    "        history = model.fit(\n",
    "            X_tr, y_tr,\n",
    "            validation_data=(X_va, y_va),\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            verbose=0,\n",
    "            callbacks=callbacks\n",
    "        )\n",
    "        return float(min(history.history[\"val_loss\"]))\n",
    "    return objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f5c5295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- Deterministic Predictions ---------------------------\n",
    "def predict_series(model, X_block, idx_block, y_scaler):\n",
    "    yhat_s = model.predict(X_block, verbose=0)\n",
    "    yhat = y_scaler.inverse_transform(yhat_s).squeeze()\n",
    "    return pd.Series(yhat, index=idx_block, name=\"Pred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3cd4796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------- MC Dropout ---------------------------\n",
    "@tf.function\n",
    "def mc_call(m, X, training=True):\n",
    "    # keep dropout active during inference\n",
    "    return m(X, training=training)\n",
    "\n",
    "def predict_mc(model, X_np, idx, y_mean: float, y_scale: float,\n",
    "               n_mc: int = 100, use_quantiles: bool = True, alpha: float = 0.10):\n",
    "    Ys_scaled = []\n",
    "    X_tf = tf.convert_to_tensor(X_np, dtype=tf.float32)\n",
    "    for _ in range(n_mc):\n",
    "        y_s = mc_call(model, X_tf, training=True).numpy().squeeze()  # (N,)\n",
    "        Ys_scaled.append(y_s)\n",
    "    Ys_scaled = np.stack(Ys_scaled, axis=1)  # (N, T)\n",
    "    Ys = Ys_scaled * y_scale + y_mean        # inverse scale: y = z*scale + mean\n",
    "\n",
    "    mean = Ys.mean(axis=1)\n",
    "    std  = Ys.std(axis=1, ddof=1)\n",
    "    if use_quantiles:\n",
    "        lower = np.quantile(Ys, q=alpha/2, axis=1)\n",
    "        upper = np.quantile(Ys, q=1 - alpha/2, axis=1)\n",
    "    else:\n",
    "        from scipy.stats import norm\n",
    "        z = norm.ppf(1 - alpha/2.0)\n",
    "        lower, upper = mean - z*std, mean + z*std\n",
    "\n",
    "    return (\n",
    "        pd.Series(mean,  index=idx, name=\"mean\"),\n",
    "        pd.Series(lower, index=idx, name=f\"lower_{int((1-alpha)*100)}\"),\n",
    "        pd.Series(upper, index=idx, name=f\"upper_{int((1-alpha)*100)}\"),\n",
    "        pd.Series(std,   index=idx, name=\"mc_std\"),\n",
    "        Ys  # raw MC draws in real scale: shape (N, T)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9e1d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed\n",
    "print(\"Seed:\", RANDOM_SEED)\n",
    "set_global_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142412c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "df = load_dataset(CSV_PATH)\n",
    "train_df, val_df, test_df = split_df(df, TEST_SIZE, VAL_SIZE)\n",
    "\n",
    "feature_cols = get_feature_cols(df, INCLUDE_TARGET_AS_FEATURE)\n",
    "X_scaler, y_scaler = fit_scalers(train_df, feature_cols)\n",
    "\n",
    "X_train_s, y_train_s = scale_block(train_df, feature_cols, X_scaler, y_scaler)\n",
    "X_val_s,   y_val_s   = scale_block(val_df,   feature_cols, X_scaler, y_scaler)\n",
    "X_test_s,  y_test_s  = scale_block(test_df,  feature_cols, X_scaler, y_scaler)\n",
    "\n",
    "y_mean, y_scale = float(y_scaler.mean_[0]), float(y_scaler.scale_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84131f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Optuna ------------------\n",
    "print(\"\\n[Optuna] Starting study...\")\n",
    "t0_hpo = time.time()\n",
    "sampler = optuna.samplers.TPESampler(seed=RANDOM_SEED)\n",
    "pruner  = optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "study   = optuna.create_study(direction=\"minimize\", sampler=sampler, pruner=pruner)\n",
    "\n",
    "objective = make_objective(X_train_s, y_train_s, X_val_s, y_val_s, n_features=len(feature_cols))\n",
    "study.optimize(objective, n_trials=N_TRIALS, show_progress_bar=USE_PROGRESS_BAR)\n",
    "best_params = study.best_params\n",
    "BEST_LOOKBACK = best_params[\"lookback\"]\n",
    "t1_hpo = time.time()\n",
    "print(f\"[Optuna] Study completed in {t1_hpo - t0_hpo:.4f}s\")\n",
    "print(\"[Optuna] Best params:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a9a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Windows with best lookback ------------------\n",
    "X_train_w, y_train_w, idx_train = make_windows(X_train_s, y_train_s, BEST_LOOKBACK)\n",
    "X_val_w,   y_val_w,   idx_val   = make_windows(X_val_s,   y_val_s,   BEST_LOOKBACK)\n",
    "X_test_w,  y_test_w,  idx_test  = make_windows(X_test_s,  y_test_s,  BEST_LOOKBACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9af14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Final deterministic training ------------------\n",
    "final_model = build_lstm_fixed(best_params, BEST_LOOKBACK, len(feature_cols))\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=best_params[\"patience\"], restore_best_weights=True),\n",
    "    ModelCheckpoint(CKPT_PATH, monitor=\"val_loss\", save_best_only=True)\n",
    "]\n",
    "\n",
    "print(\"\\n[Train] Retraining final LSTM with best params...\")\n",
    "t0_train = time.time()\n",
    "history = final_model.fit(\n",
    "    X_train_w, y_train_w,\n",
    "    validation_data=(X_val_w, y_val_w),\n",
    "    epochs=best_params[\"epochs\"],\n",
    "    batch_size=best_params[\"batch_size\"],\n",
    "    verbose=1,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "t1_train = time.time()\n",
    "print(f\"[Train] Done in {t1_train-t0_train:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Deterministic predictions (Stage 1 comparability) ------------------\n",
    "actual = df[TARGET_COL]\n",
    "actual_train = actual.loc[idx_train]\n",
    "actual_val   = actual.loc[idx_val]\n",
    "actual_test  = actual.loc[idx_test]\n",
    "\n",
    "t0_test = time.time()\n",
    "pred_train_det = predict_series(final_model, X_train_w, idx_train, y_scaler)\n",
    "pred_val_det   = predict_series(final_model, X_val_w,   idx_val,   y_scaler)\n",
    "pred_test_det  = predict_series(final_model, X_test_w,  idx_test,  y_scaler)\n",
    "t1_test = time.time()\n",
    "print(f\"[Test Predictions] Done in {t1_test - t0_test:.4f}s\")\n",
    "\n",
    "print(\"\\n=== Stage-1 Point Forecast Metrics (same model used later for MC) ===\")\n",
    "print(\"Train:\", base_metrics(actual_train.values, pred_train_det.values))\n",
    "print(\"Val:  \", base_metrics(actual_val.values,   pred_val_det.values))\n",
    "print(\"Test: \", base_metrics(actual_test.values,  pred_test_det.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d72d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Load the SAME trained model for MC ------------------\n",
    "same_model = tf.keras.models.load_model(CKPT_PATH, compile=False)\n",
    "same_model.compile(optimizer=optimizers.Adam(learning_rate=best_params[\"lr\"]), loss=\"mse\")\n",
    "\n",
    "print(\"\\n[MC] Running Monte Carlo Dropout inference with same trained weights...\")\n",
    "t0_mcd = time.time()    \n",
    "mean_train, L_train, U_train, std_train, Ys_train = predict_mc(\n",
    "    same_model, X_train_w, idx_train, y_mean, y_scale, n_mc=N_MC, use_quantiles=USE_QUANTILES, alpha=ALPHA\n",
    ")\n",
    "mean_val,   L_val,   U_val,   std_val,   Ys_val   = predict_mc(\n",
    "    same_model, X_val_w,   idx_val,   y_mean, y_scale, n_mc=N_MC, use_quantiles=USE_QUANTILES, alpha=ALPHA\n",
    ")\n",
    "mean_test,  L_test,  U_test,  std_test,  Ys_test  = predict_mc(\n",
    "    same_model, X_test_w,  idx_test,  y_mean, y_scale, n_mc=N_MC, use_quantiles=USE_QUANTILES, alpha=ALPHA\n",
    ")\n",
    "t1_mcd = time.time()\n",
    "print(f\"[MC] Done in {t1_mcd - t0_mcd:.4f}s\")\n",
    "\n",
    "# Stage-2 Point Forecast Metrics (after MC Dropout)\n",
    "print(f\"\\n=== Stage-2 Point Forecast Metrics (MC Mean) ===\")\n",
    "print(\"Train:\", base_metrics(actual_train.values, mean_train.values))\n",
    "print(\"Val:  \", base_metrics(actual_val.values,   mean_val.values))\n",
    "print(\"Test: \", base_metrics(actual_test.values,  mean_test.values))\n",
    "\n",
    "print(f\"\\n=== Stage-2 UQ Metrics ({int((1-ALPHA)*100)}% PI) ===\")\n",
    "print(\"Train:\", uq_metrics(actual_train.values, L_train.values, U_train.values, ALPHA))\n",
    "print(\"Val:  \", uq_metrics(actual_val.values,   L_val.values,   U_val.values,   ALPHA))\n",
    "print(\"Test: \", uq_metrics(actual_test.values,  L_test.values,  U_test.values,  ALPHA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666af70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Epistemic vs Aleatoric (approximate split) ------------------\n",
    "resid_val = actual_val.values - mean_val.values\n",
    "sigma2_aleatoric = float(np.var(resid_val, ddof=1))\n",
    "var_total_test   = np.var(Ys_test, axis=1, ddof=1)\n",
    "var_epistemic    = np.maximum(0.0, var_total_test - sigma2_aleatoric)\n",
    "var_aleatoric    = np.full_like(var_total_test, sigma2_aleatoric)\n",
    "\n",
    "# Save artifacts\n",
    "best_json_path = os.path.join(RESULTS_DIR, \"lstm_best_params.json\")\n",
    "with open(best_json_path, \"w\") as f:\n",
    "    json.dump(best_params, f, indent=2)\n",
    "print(f\"\\nSaved best params to: {best_json_path}\")\n",
    "print(f\"Saved trained model to: {CKPT_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4364272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Visualizations ------------------\n",
    "if DO_PLOTS:\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(df.index, df[TARGET_COL].values, label=\"Actual\", linewidth=1)\n",
    "    plt.plot(mean_train.index, mean_train.values, label=\"Pred (Train, MC mean)\", linewidth=1)\n",
    "    plt.plot(mean_val.index,   mean_val.values,   label=\"Pred (Val, MC mean)\", linewidth=1)\n",
    "    plt.plot(mean_test.index,  mean_test.values,  label=\"Pred (Test, MC mean)\", linewidth=1.5)\n",
    "    plt.fill_between(L_test.index, L_test.values, U_test.values, alpha=0.25, label=f\"{int((1-ALPHA)*100)}% PI (Test)\")\n",
    "    plt.title(\"All Actual vs Predicted â€” LSTM (Optuna) + MC Dropout\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(TARGET_COL); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # inside/outside markers on Test\n",
    "    y_true = actual_test.values; L_arr = L_test.values; U_arr = U_test.values\n",
    "    inside = (y_true >= L_arr) & (y_true <= U_arr)\n",
    "    outside = ~inside\n",
    "\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(actual_test.index, actual_test.values, label=\"Actual (Test)\", linewidth=1.5)\n",
    "    plt.plot(mean_test.index,   mean_test.values,   label=\"Pred (Test, MC mean)\", linewidth=1.5)\n",
    "    plt.fill_between(L_test.index, L_test.values, U_test.values, alpha=0.30, label=f\"{int((1-ALPHA)*100)}% PI\")\n",
    "    plt.scatter(actual_test.index[inside],  actual_test.values[inside],  s=15, label=\"Inside PI\", zorder=3, color=\"limegreen\")\n",
    "    plt.scatter(actual_test.index[outside], actual_test.values[outside], s=25, marker=\"x\", label=\"Outside PI\", zorder=3, color=\"red\")\n",
    "    plt.title(\"Test Horizon: Actual vs MC Mean with Prediction Interval (+ inside/outside)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(TARGET_COL); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # residuals\n",
    "    residuals_test = pd.Series(actual_test.values - mean_test.values, index=actual_test.index, name=\"Residuals\")\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(residuals_test.index, residuals_test.values, linewidth=1)\n",
    "    plt.axhline(0, ls=\"--\", lw=1); plt.title(\"Residuals Over Time (Test) â€” LSTM + MC\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Actual - Pred\"); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # uncertainty decomposition\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.plot(idx_test, var_epistemic, label=\"Epistemic Var (â‰ˆ)\", linewidth=1)\n",
    "    plt.plot(idx_test, var_aleatoric, label=\"Aleatoric Var (proxy)\", linewidth=1)\n",
    "    plt.plot(idx_test, var_total_test, label=\"Total Predictive Var (MC)\", linewidth=1.2)\n",
    "    plt.title(\"Uncertainty Decomposition Over Time (Test)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Variance\"); plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ===================== (a) Coverage heatmap (per-window) =====================\n",
    "    from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "    WINDOW_LEN = 30\n",
    "    STRIDE     = 10\n",
    "\n",
    "    y_true = actual_test.values\n",
    "    L_arr  = L_test.values\n",
    "    U_arr  = U_test.values\n",
    "    below_mask  = (y_true < L_arr)\n",
    "    above_mask  = (y_true > U_arr)\n",
    "    inside_mask = (y_true >= L_arr) & (y_true <= U_arr)\n",
    "\n",
    "    status = np.zeros_like(y_true, dtype=int)\n",
    "    status[below_mask] = -1\n",
    "    status[above_mask] =  1\n",
    "\n",
    "    starts = np.arange(0, len(status)-WINDOW_LEN+1, STRIDE)\n",
    "    if len(starts) == 0:\n",
    "        starts = np.array([0]); WINDOW_LEN = len(status)\n",
    "\n",
    "    mat = []\n",
    "    x_tick_labels = []\n",
    "    for s in starts:\n",
    "        e = min(s + WINDOW_LEN, len(status))\n",
    "        row = status[s:e]\n",
    "        if e - s < WINDOW_LEN:\n",
    "            row = np.pad(row, (0, WINDOW_LEN - (e - s)), constant_values=np.nan)\n",
    "        mat.append(row)\n",
    "        x_tick_labels.append(actual_test.index[s].strftime('%Y-%m-%d'))\n",
    "    mat = np.vstack(mat)\n",
    "\n",
    "    cmap   = ListedColormap([\"#d62728\", \"#2ca02c\", \"#ff7f0e\", \"#bdbdbd\"])  # red, green, orange, grey\n",
    "    bounds = [-1.5, -0.5, 0.5, 1.5, 2.5]\n",
    "    norm   = BoundaryNorm(bounds, cmap.N)\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.imshow(mat, aspect=\"auto\", interpolation=\"nearest\", cmap=cmap, norm=norm)\n",
    "    plt.title(f\"Coverage Heatmap (Test) â€” window={WINDOW_LEN}, stride={STRIDE}\\n-1: Below | 0: Inside | +1: Above\")\n",
    "    plt.xlabel(\"Position inside window\"); plt.ylabel(\"Window start time\")\n",
    "    yticks = np.arange(0, len(starts), max(1, len(starts)//10))\n",
    "    plt.yticks(yticks, [x_tick_labels[i] for i in yticks])\n",
    "    import matplotlib.patches as mpatches\n",
    "    legend_patches = [\n",
    "        mpatches.Patch(color=\"#2ca02c\", label=\"Inside PI\"),\n",
    "        mpatches.Patch(color=\"#d62728\", label=\"Below lower\"),\n",
    "        mpatches.Patch(color=\"#ff7f0e\", label=\"Above upper\"),\n",
    "        mpatches.Patch(color=\"#bdbdbd\", label=\"Padding\")\n",
    "    ]\n",
    "    plt.legend(handles=legend_patches, loc=\"upper right\", frameon=True)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ===================== Rolling PICP (calibration drift) =====================\n",
    "    ROLL_LEN = 30\n",
    "    y_true = actual_test.values; L_arr = L_test.values; U_arr = U_test.values\n",
    "    inside_series = pd.Series(((y_true >= L_arr) & (y_true <= U_arr)).astype(int), index=actual_test.index, name=\"inside\")\n",
    "    rolling_picp = inside_series.rolling(window=ROLL_LEN, center=True, min_periods=max(3, ROLL_LEN//3)).mean()\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(rolling_picp.index, rolling_picp.values, linewidth=1.8, label=f\"Rolling PICP (window={ROLL_LEN})\")\n",
    "    plt.axhline(1 - ALPHA, ls=\"--\", lw=1, label=f\"Target coverage = {1-ALPHA:.2f}\")\n",
    "    plt.ylim(0, 1.05)\n",
    "    plt.title(\"Rolling PICP on Test (Calibration over Time)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Coverage\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ===================== Rolling MPIW (sharpness drift) =====================\n",
    "    width_series = pd.Series((U_arr - L_arr), index=actual_test.index, name=\"PI_width\")\n",
    "    rolling_mpiw = width_series.rolling(window=ROLL_LEN, center=True, min_periods=max(3, ROLL_LEN//3)).mean()\n",
    "    overall_mpiw_test = width_series.mean()\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(rolling_mpiw.index, rolling_mpiw.values, linewidth=1.8, label=f\"Rolling MPIW (window={ROLL_LEN})\")\n",
    "    plt.axhline(overall_mpiw_test, ls=\"--\", lw=1, label=f\"Overall MPIW (Test) = {overall_mpiw_test:.2f}\")\n",
    "    plt.title(\"Rolling MPIW on Test (Sharpness over Time)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Interval Width\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ===================== Rolling PICP vs Normalized MPIW (combined) =====================\n",
    "    norm_mpiw = (rolling_mpiw - np.nanmin(rolling_mpiw)) / (np.nanmax(rolling_mpiw) - np.nanmin(rolling_mpiw) + 1e-12)\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(rolling_picp.index, rolling_picp.values, linewidth=1.8, label=\"Rolling PICP (0â€“1)\")\n",
    "    plt.plot(norm_mpiw.index, norm_mpiw.values, linewidth=1.5, label=\"Rolling MPIW (normalized 0â€“1)\")\n",
    "    plt.axhline(1 - ALPHA, ls=\"--\", lw=1, label=f\"Target coverage = {1-ALPHA:.2f}\")\n",
    "    plt.title(\"Rolling PICP vs Normalized Rolling MPIW (Test)\")\n",
    "    plt.xlabel(\"Date\"); plt.ylabel(\"Scaled value\")\n",
    "    plt.legend(); plt.tight_layout(); plt.show()\n",
    "\n",
    "    # ===================== Stage-1 vs Stage-2 Metrics Comparison =====================\n",
    "    # Calculate Stage-1 metrics (deterministic)\n",
    "    stage1_train = base_metrics(actual_train.values, pred_train_det.values)\n",
    "    stage1_val   = base_metrics(actual_val.values,   pred_val_det.values)\n",
    "    stage1_test  = base_metrics(actual_test.values,  pred_test_det.values)\n",
    "\n",
    "    # Stage-2 metrics already computed (MC mean)\n",
    "    stage2_train = base_metrics(actual_train.values, mean_train.values)\n",
    "    stage2_val   = base_metrics(actual_val.values,   mean_val.values)\n",
    "    stage2_test  = base_metrics(actual_test.values,  mean_test.values)\n",
    "\n",
    "    # Prepare data for plotting\n",
    "    metrics_names = [\"MAE\", \"MSE\", \"MAPE\", \"RMSE\", \"R2\"]\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for idx, metric in enumerate(metrics_names):\n",
    "        ax = axes[idx]\n",
    "        stage1_vals = [stage1_train[metric], stage1_val[metric], stage1_test[metric]]\n",
    "        stage2_vals = [stage2_train[metric], stage2_val[metric], stage2_test[metric]]\n",
    "\n",
    "        x = np.arange(3)\n",
    "        width = 0.35\n",
    "\n",
    "        bars1 = ax.bar(x - width/2, stage1_vals, width, label=\"Stage-1 (Deterministic)\", alpha=0.8, color=\"#1f77b4\")\n",
    "        bars2 = ax.bar(x + width/2, stage2_vals, width, label=\"Stage-2 (MC Mean)\", alpha=0.8, color=\"#ff7f0e\")\n",
    "\n",
    "        ax.set_xlabel(\"Split\")\n",
    "        ax.set_ylabel(metric)\n",
    "        ax.set_title(f\"{metric} Comparison: Stage-1 vs Stage-2\")\n",
    "        ax.set_xticks(x)\n",
    "        ax.set_xticklabels([\"Train\", \"Val\", \"Test\"])\n",
    "        ax.legend()\n",
    "        ax.grid(axis=\"y\", alpha=0.3)\n",
    "\n",
    "        # Add value labels on bars\n",
    "        for bar in bars1:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "        for bar in bars2:\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{height:.4f}', ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "    # Remove the extra subplot\n",
    "    fig.delaxes(axes[-1])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary table\n",
    "    print(\"\\n=== Stage-1 vs Stage-2 Point Forecast Metrics Comparison ===\")\n",
    "    comparison_data = []\n",
    "    for split, stage1, stage2 in [\n",
    "        (\"Train\", stage1_train, stage2_train),\n",
    "        (\"Val\", stage1_val, stage2_val),\n",
    "        (\"Test\", stage1_test, stage2_test),\n",
    "    ]:\n",
    "        for metric in metrics_names:\n",
    "            diff = stage2[metric] - stage1[metric]\n",
    "            pct_change = (diff / stage1[metric] * 100) if stage1[metric] != 0 else 0\n",
    "            comparison_data.append({\n",
    "                \"Split\": split,\n",
    "                \"Metric\": metric,\n",
    "                \"Stage-1\": round(stage1[metric], 6),\n",
    "                \"Stage-2\": round(stage2[metric], 6),\n",
    "                \"Difference\": round(diff, 6),\n",
    "                \"% Change\": round(pct_change, 2)\n",
    "            })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aea5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "results_dir = os.path.join(\"..\", \"Results\")\n",
    "predicted_path = os.path.join(results_dir, \"ALL_UQ_PREDICTED.csv\")\n",
    "metrics_path = os.path.join(results_dir, \"ALL_UQ_METRICS.csv\")\n",
    "\n",
    "# Manual model name (since __file__ isn't available in notebooks)\n",
    "model = \"lstm_mcd\"\n",
    "model_name = f\"{model}_{RANDOM_SEED}\"   # change this for each notebook (e.g., GRU_Baseline)\n",
    "print(\"Model Name for Documentation:\", model_name)\n",
    "\n",
    "# Create Results directory if not exists\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# ==========================================\n",
    "# 1ï¸âƒ£ PREPARE AND ALIGN TESTING DATAFRAME\n",
    "# ==========================================\n",
    "\n",
    "# Convert dates\n",
    "test_dates = test_df.index.to_series().reset_index(drop=True)\n",
    "actual_values = test_df[TARGET_COL].values\n",
    "\n",
    "# If ALL_PREDICTED doesn't exist, create the base file\n",
    "if not os.path.exists(predicted_path):\n",
    "    print(\"Creating ALL_PREDICTED.csv ...\")\n",
    "    base_df = pd.DataFrame({\n",
    "        \"date\": test_dates,\n",
    "        \"actual\": actual_values\n",
    "    })\n",
    "    base_df.to_csv(predicted_path, index=False)\n",
    "\n",
    "# Load and ensure datetime consistency\n",
    "all_pred_df = pd.read_csv(predicted_path)\n",
    "all_pred_df[\"date\"] = pd.to_datetime(all_pred_df[\"date\"])\n",
    "\n",
    "# Ensure the file covers full test range (in case it was made from smaller data)\n",
    "base_df = pd.DataFrame({\n",
    "    \"date\": test_dates,\n",
    "    \"actual\": actual_values\n",
    "})\n",
    "# Outer merge to make sure we have the full timeline\n",
    "all_pred_df = pd.merge(base_df, all_pred_df, on=[\"date\", \"actual\"], how=\"outer\")\n",
    "\n",
    "# Create new prediction column (aligned to date)\n",
    "pred_series = pd.Series(mean_test.values, index=pd.to_datetime(idx_test), name=model_name)\n",
    "pred_series = pred_series.reindex(all_pred_df[\"date\"])  # align by date\n",
    "\n",
    "# Create lower and upper bound series (aligned to date)\n",
    "L_series = pd.Series(L_test.values, index=pd.to_datetime(idx_test), name=f\"{model_name}_L\")\n",
    "U_series = pd.Series(U_test.values, index=pd.to_datetime(idx_test), name=f\"{model_name}_U\")\n",
    "L_series = L_series.reindex(all_pred_df[\"date\"])  # align by date\n",
    "U_series = U_series.reindex(all_pred_df[\"date\"])  # align by date\n",
    "\n",
    "# Add or update the model column\n",
    "all_pred_df[model_name] = pred_series.values\n",
    "all_pred_df[f\"{model_name}_L\"] = L_series.values  # Add lower bound\n",
    "all_pred_df[f\"{model_name}_U\"] = U_series.values  # Add upper bound\n",
    "\n",
    "# Sort and save\n",
    "all_pred_df = all_pred_df.sort_values(\"date\").reset_index(drop=True)\n",
    "all_pred_df.to_csv(predicted_path, index=False)\n",
    "print(f\"âœ… Predictions saved to {predicted_path}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2ï¸âƒ£ RECORD METRICS SUMMARY\n",
    "# ==========================================\n",
    "metrics_columns = [\n",
    "    \"timestamp\",\n",
    "    \"model_name\", \n",
    "    \"seed\",\n",
    "    \"mse\",\n",
    "    \"mae\",\n",
    "    \"rmse\", \n",
    "    \"mape\",\n",
    "    \"r2_score\",\n",
    "    \"picp\",\n",
    "    \"mpiw\",\n",
    "    \"winkler_score\",\n",
    "    \"training_time_s\",\n",
    "    \"testing_time_s\", \n",
    "    \"hpo_trial_s\",\n",
    "    \"hpo_time_s\"\n",
    "]\n",
    "\n",
    "# Create ALL_METRICS if missing\n",
    "if not os.path.exists(metrics_path):\n",
    "    print(\"Creating ALL_METRICS.csv ...\")\n",
    "    pd.DataFrame(columns=metrics_columns).to_csv(metrics_path, index=False)\n",
    "\n",
    "# Current timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "# Extract metrics\n",
    "mse, mae, rmse, mape, r2 = base_metrics(actual_test.values, mean_test.values).values()\n",
    "picp, mpiw, winkler = uq_metrics(actual_test.values, L_test.values, U_test.values, ALPHA).values()\n",
    "\n",
    "# Build metrics row\n",
    "metrics_row = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"model_name\": model_name,\n",
    "    \"seed\": RANDOM_SEED,\n",
    "    \"mse\": mse,\n",
    "    \"mae\": mae,\n",
    "    \"rmse\": rmse,\n",
    "    \"mape\": mape,\n",
    "    \"r2_score\": r2,\n",
    "    \"picp\": picp,\n",
    "    \"mpiw\": mpiw,\n",
    "    \"winkler_score\": winkler,\n",
    "    \"training_time_s\": round(t1_train - t0_train, 4),\n",
    "    \"testing_time_s\": round(t1_mcd - t0_mcd, 4),\n",
    "    \"hpo_trial_s\": N_TRIALS,\n",
    "    \"hpo_time_s\": round(t1_hpo - t0_hpo, 4),\n",
    "}\n",
    "\n",
    "# Append metrics\n",
    "all_metrics_df = pd.read_csv(metrics_path)\n",
    "all_metrics_df = pd.concat([all_metrics_df, pd.DataFrame([metrics_row])], ignore_index=True)\n",
    "all_metrics_df.to_csv(metrics_path, index=False)\n",
    "print(f\"âœ… Metrics appended to {metrics_path}\")\n",
    "\n",
    "print(\"\\nğŸ“„ Documentation of predictions and metrics completed successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tesis_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
